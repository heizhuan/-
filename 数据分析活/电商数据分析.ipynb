{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# 电商数据分析项目\n",
        "\n",
        "本项目使用大数据技术对电商数据进行全面分析，共分为9个部分：\n",
        "\n",
        "1. 数据生成和准备\n",
        "2. 基础销售分析\n",
        "3. 用户行为分析\n",
        "4. 商品分类分析\n",
        "5. RFM客户价值分析\n",
        "6. 商品关联规则分析\n",
        "7. 地理位置分析\n",
        "8. 营销活动分析\n",
        "9. 实时数据处理\n",
        "\n",
        "## 第一部分：数据生成和准备\n",
        "\n",
        "首先我们需要生成模拟的电商数据，包括：\n",
        "- 用户信息\n",
        "- 商品信息\n",
        "- 订单数据\n",
        "- 用户行为数据\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import random\n",
        "\n",
        "# 设置随机种子\n",
        "np.random.seed(42)\n",
        "\n",
        "# 生成用户数据\n",
        "def generate_users(n_users=1000):\n",
        "    provinces = ['北京', '上海', '广东', '江苏', '浙江', '四川', '湖北', '河南', '山东', '河北']\n",
        "    age_ranges = ['18-24', '25-34', '35-44', '45-54', '55+']\n",
        "    \n",
        "    users = {\n",
        "        'user_id': range(1, n_users + 1),\n",
        "        'age_range': np.random.choice(age_ranges, n_users),\n",
        "        'gender': np.random.choice(['M', 'F'], n_users),\n",
        "        'province': np.random.choice(provinces, n_users),\n",
        "        'register_date': [datetime(2023, 1, 1) + timedelta(days=np.random.randint(0, 365)) for _ in range(n_users)]\n",
        "    }\n",
        "    return pd.DataFrame(users)\n",
        "\n",
        "# 生成商品数据\n",
        "def generate_products(n_products=100):\n",
        "    categories = ['电子产品', '服装', '食品', '家居', '美妆']\n",
        "    products = {\n",
        "        'product_id': range(1, n_products + 1),\n",
        "        'category': np.random.choice(categories, n_products),\n",
        "        'price': np.random.uniform(10, 1000, n_products).round(2),\n",
        "        'stock': np.random.randint(0, 1000, n_products)\n",
        "    }\n",
        "    return pd.DataFrame(products)\n",
        "\n",
        "# 生成订单数据\n",
        "def generate_orders(users_df, products_df, n_orders=5000):\n",
        "    orders = []\n",
        "    for _ in range(n_orders):\n",
        "        user_id = np.random.choice(users_df['user_id'])\n",
        "        n_items = np.random.randint(1, 5)\n",
        "        order_date = datetime(2023, 1, 1) + timedelta(days=np.random.randint(0, 365))\n",
        "        \n",
        "        for _ in range(n_items):\n",
        "            product_id = np.random.choice(products_df['product_id'])\n",
        "            product_price = products_df.loc[products_df['product_id'] == product_id, 'price'].iloc[0]\n",
        "            quantity = np.random.randint(1, 5)\n",
        "            \n",
        "            orders.append({\n",
        "                'order_id': len(orders) + 1,\n",
        "                'user_id': user_id,\n",
        "                'product_id': product_id,\n",
        "                'quantity': quantity,\n",
        "                'price': product_price,\n",
        "                'total_amount': quantity * product_price,\n",
        "                'order_date': order_date\n",
        "            })\n",
        "    \n",
        "    return pd.DataFrame(orders)\n",
        "\n",
        "# 生成数据\n",
        "users_df = generate_users()\n",
        "products_df = generate_products()\n",
        "orders_df = generate_orders(users_df, products_df)\n",
        "\n",
        "# 显示数据预览\n",
        "print(\"用户数据预览：\")\n",
        "print(users_df.head())\n",
        "print(\"\\n商品数据预览：\")\n",
        "print(products_df.head())\n",
        "print(\"\\n订单数据预览：\")\n",
        "print(orders_df.head())\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 第二部分：基础销售分析\n",
        "\n",
        "在这部分，我们将分析：\n",
        "- 总体销售情况\n",
        "- 日销售趋势\n",
        "- 月度销售对比\n",
        "- 销售额分布\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "\n",
        "# 设置中文显示\n",
        "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "# 1. 总体销售情况\n",
        "total_sales = orders_df['total_amount'].sum()\n",
        "total_orders = len(orders_df['order_id'].unique())\n",
        "avg_order_value = total_sales / total_orders\n",
        "\n",
        "print(f\"总销售额: ¥{total_sales:,.2f}\")\n",
        "print(f\"总订单数: {total_orders:,}\")\n",
        "print(f\"平均订单金额: ¥{avg_order_value:.2f}\")\n",
        "\n",
        "# 2. 日销售趋势\n",
        "daily_sales = orders_df.groupby('order_date')['total_amount'].sum().reset_index()\n",
        "\n",
        "plt.figure(figsize=(15, 6))\n",
        "plt.plot(daily_sales['order_date'], daily_sales['total_amount'])\n",
        "plt.title('日销售趋势')\n",
        "plt.xlabel('日期')\n",
        "plt.ylabel('销售额')\n",
        "plt.grid(True)\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 3. 月度销售对比\n",
        "orders_df['month'] = orders_df['order_date'].dt.month\n",
        "monthly_sales = orders_df.groupby('month')['total_amount'].sum().reset_index()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(monthly_sales['month'], monthly_sales['total_amount'])\n",
        "plt.title('月度销售对比')\n",
        "plt.xlabel('月份')\n",
        "plt.ylabel('销售额')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# 4. 销售额分布\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(orders_df['total_amount'], bins=50)\n",
        "plt.title('订单金额分布')\n",
        "plt.xlabel('订单金额')\n",
        "plt.ylabel('频次')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# 基础统计量\n",
        "print(\"\\n销售额基础统计：\")\n",
        "print(orders_df['total_amount'].describe())\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 第三部分：用户行为分析\n",
        "\n",
        "分析用户的购买行为特征：\n",
        "- 用户活跃度分析\n",
        "- 用户购买频率\n",
        "- 用户消费水平\n",
        "- 用户画像分析\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. 用户活跃度分析\n",
        "user_activity = orders_df.groupby('user_id').agg({\n",
        "    'order_id': 'count',\n",
        "    'total_amount': 'sum',\n",
        "    'order_date': lambda x: (x.max() - x.min()).days + 1\n",
        "}).rename(columns={\n",
        "    'order_id': '购买次数',\n",
        "    'total_amount': '总消费额',\n",
        "    'order_date': '活跃天数'\n",
        "})\n",
        "\n",
        "# 2. 用户购买频率分布\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(user_activity['购买次数'], bins=30)\n",
        "plt.title('用户购买频率分布')\n",
        "plt.xlabel('购买次数')\n",
        "plt.ylabel('用户数')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# 3. 用户消费水平分析\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(user_activity['总消费额'], bins=30)\n",
        "plt.title('用户消费水平分布')\n",
        "plt.xlabel('总消费额')\n",
        "plt.ylabel('用户数')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# 4. 用户画像分析\n",
        "# 合并用户信息\n",
        "user_profile = pd.merge(user_activity, users_df, on='user_id')\n",
        "\n",
        "# 按年龄段分析\n",
        "age_analysis = user_profile.groupby('age_range').agg({\n",
        "    '总消费额': 'mean',\n",
        "    '购买次数': 'mean'\n",
        "}).round(2)\n",
        "\n",
        "print(\"\\n不同年龄段用户行为：\")\n",
        "print(age_analysis)\n",
        "\n",
        "# 按性别分析\n",
        "gender_analysis = user_profile.groupby('gender').agg({\n",
        "    '总消费额': 'mean',\n",
        "    '购买次数': 'mean'\n",
        "}).round(2)\n",
        "\n",
        "print(\"\\n不同性别用户行为：\")\n",
        "print(gender_analysis)\n",
        "\n",
        "# 可视化地理分布\n",
        "plt.figure(figsize=(12, 6))\n",
        "province_sales = user_profile.groupby('province')['总消费额'].sum().sort_values(ascending=False)\n",
        "province_sales.plot(kind='bar')\n",
        "plt.title('各省份销售额分布')\n",
        "plt.xlabel('省份')\n",
        "plt.ylabel('总销售额')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 第四部分：商品分类分析\n",
        "\n",
        "分析不同商品类别的表现：\n",
        "- 类别销售额占比\n",
        "- 类别销量分析\n",
        "- 商品价格分布\n",
        "- 热销商品分析\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 合并订单和商品数据\n",
        "order_products = pd.merge(orders_df, products_df, on='product_id')\n",
        "\n",
        "# 1. 类别销售额占比\n",
        "category_sales = order_products.groupby('category')['total_amount'].sum()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.pie(category_sales, labels=category_sales.index, autopct='%1.1f%%')\n",
        "plt.title('各类别销售额占比')\n",
        "plt.axis('equal')\n",
        "plt.show()\n",
        "\n",
        "# 2. 类别销量分析\n",
        "category_quantity = order_products.groupby('category')['quantity'].sum()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(category_quantity.index, category_quantity.values)\n",
        "plt.title('各类别销量分析')\n",
        "plt.xlabel('商品类别')\n",
        "plt.ylabel('销量')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# 3. 商品价格分布\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(x='category', y='price', data=products_df)\n",
        "plt.title('各类别商品价格分布')\n",
        "plt.xlabel('商品类别')\n",
        "plt.ylabel('价格')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# 4. 热销商品分析\n",
        "top_products = order_products.groupby('product_id').agg({\n",
        "    'quantity': 'sum',\n",
        "    'total_amount': 'sum',\n",
        "    'category': 'first'\n",
        "}).sort_values('total_amount', ascending=False).head(10)\n",
        "\n",
        "print(\"\\n销售额Top10商品：\")\n",
        "print(top_products)\n",
        "\n",
        "# 计算类别均价和销售额\n",
        "category_stats = order_products.groupby('category').agg({\n",
        "    'price': 'mean',\n",
        "    'total_amount': 'sum',\n",
        "    'quantity': 'sum'\n",
        "}).round(2)\n",
        "\n",
        "print(\"\\n类别统计：\")\n",
        "print(category_stats)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 第五部分：RFM客户价值分析\n",
        "\n",
        "使用RFM模型分析客户价值：\n",
        "- R (Recency): 最近一次购买时间\n",
        "- F (Frequency): 购买频率\n",
        "- M (Monetary): 购买金额\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# 计算RFM指标\n",
        "now = orders_df['order_date'].max()\n",
        "\n",
        "rfm = orders_df.groupby('user_id').agg({\n",
        "    'order_date': lambda x: (now - x.max()).days,  # Recency\n",
        "    'order_id': 'count',  # Frequency\n",
        "    'total_amount': 'sum'  # Monetary\n",
        "}).rename(columns={\n",
        "    'order_date': 'recency',\n",
        "    'order_id': 'frequency',\n",
        "    'total_amount': 'monetary'\n",
        "})\n",
        "\n",
        "# 标准化RFM指标\n",
        "scaler = StandardScaler()\n",
        "rfm_normalized = scaler.fit_transform(rfm)\n",
        "\n",
        "# 使用K-means进行客户分群\n",
        "kmeans = KMeans(n_clusters=4, random_state=42)\n",
        "rfm['cluster'] = kmeans.fit_predict(rfm_normalized)\n",
        "\n",
        "# 分析各群体特征\n",
        "cluster_analysis = rfm.groupby('cluster').agg({\n",
        "    'recency': 'mean',\n",
        "    'frequency': 'mean',\n",
        "    'monetary': 'mean',\n",
        "    'user_id': 'count'\n",
        "}).round(2)\n",
        "\n",
        "print(\"客户群体特征：\")\n",
        "print(cluster_analysis)\n",
        "\n",
        "# 可视化客户群体\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# 1. Recency vs Monetary\n",
        "plt.subplot(131)\n",
        "plt.scatter(rfm['recency'], rfm['monetary'], c=rfm['cluster'], cmap='viridis')\n",
        "plt.xlabel('Recency (days)')\n",
        "plt.ylabel('Monetary')\n",
        "plt.title('Recency vs Monetary')\n",
        "\n",
        "# 2. Frequency vs Monetary\n",
        "plt.subplot(132)\n",
        "plt.scatter(rfm['frequency'], rfm['monetary'], c=rfm['cluster'], cmap='viridis')\n",
        "plt.xlabel('Frequency')\n",
        "plt.ylabel('Monetary')\n",
        "plt.title('Frequency vs Monetary')\n",
        "\n",
        "# 3. Recency vs Frequency\n",
        "plt.subplot(133)\n",
        "plt.scatter(rfm['recency'], rfm['frequency'], c=rfm['cluster'], cmap='viridis')\n",
        "plt.xlabel('Recency (days)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Recency vs Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 计算各群体占比\n",
        "cluster_proportions = (rfm['cluster'].value_counts() / len(rfm) * 100).round(2)\n",
        "print(\"\\n客户群体占比：\")\n",
        "print(cluster_proportions)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 第六部分：商品关联规则分析\n",
        "\n",
        "使用Apriori算法分析商品之间的关联关系：\n",
        "- 频繁项集挖掘\n",
        "- 关联规则生成\n",
        "- 支持度和置信度分析\n",
        "- 商品组合推荐\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from mlxtend.frequent_patterns import apriori\n",
        "from mlxtend.frequent_patterns import association_rules\n",
        "\n",
        "# 准备数据\n",
        "# 将订单-商品数据转换为交易矩阵\n",
        "transaction_matrix = pd.crosstab(orders_df['order_id'], orders_df['product_id'])\n",
        "transaction_matrix = (transaction_matrix > 0).astype(int)\n",
        "\n",
        "# 使用Apriori算法找出频繁项集\n",
        "frequent_itemsets = apriori(transaction_matrix, min_support=0.01, use_colnames=True)\n",
        "\n",
        "# 生成关联规则\n",
        "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.5)\n",
        "\n",
        "# 按提升度排序\n",
        "rules = rules.sort_values('lift', ascending=False)\n",
        "\n",
        "print(\"商品关联规则（Top 10）：\")\n",
        "print(rules.head(10))\n",
        "\n",
        "# 可视化支持度和置信度的关系\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(rules['support'], rules['confidence'], alpha=0.5)\n",
        "plt.xlabel('支持度')\n",
        "plt.ylabel('置信度')\n",
        "plt.title('关联规则支持度vs置信度')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# 找出最强关联的商品组合\n",
        "top_combinations = rules.sort_values('lift', ascending=False).head(5)\n",
        "print(\"\\n最强关联的商品组合：\")\n",
        "print(top_combinations[['antecedents', 'consequents', 'lift', 'confidence']])\n",
        "\n",
        "# 可视化不同提升度的规则数量\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(rules['lift'], bins=30)\n",
        "plt.xlabel('提升度')\n",
        "plt.ylabel('规则数量')\n",
        "plt.title('关联规则提升度分布')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 第七部分：地理位置分析\n",
        "\n",
        "分析各地区的销售情况：\n",
        "- 省份销售分布\n",
        "- 地区消费能力\n",
        "- 地区商品偏好\n",
        "- 区域市场机会\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 合并订单、用户和商品数据\n",
        "geo_analysis = pd.merge(orders_df, users_df, on='user_id')\n",
        "geo_analysis = pd.merge(geo_analysis, products_df, on='product_id')\n",
        "\n",
        "# 1. 省份销售分布\n",
        "province_sales = geo_analysis.groupby('province').agg({\n",
        "    'total_amount': 'sum',\n",
        "    'order_id': 'count',\n",
        "    'user_id': 'nunique'\n",
        "}).round(2)\n",
        "\n",
        "print(\"省份销售情况：\")\n",
        "print(province_sales)\n",
        "\n",
        "# 可视化省份销售分布\n",
        "plt.figure(figsize=(12, 6))\n",
        "province_sales['total_amount'].plot(kind='bar')\n",
        "plt.title('省份销售额分布')\n",
        "plt.xlabel('省份')\n",
        "plt.ylabel('销售额')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# 2. 地区消费能力\n",
        "province_consumption = geo_analysis.groupby('province').agg({\n",
        "    'total_amount': lambda x: x.sum() / len(x.unique())  # 平均订单金额\n",
        "}).round(2)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "province_consumption['total_amount'].plot(kind='bar')\n",
        "plt.title('省份平均订单金额')\n",
        "plt.xlabel('省份')\n",
        "plt.ylabel('平均订单金额')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# 3. 地区商品偏好\n",
        "province_category = geo_analysis.groupby(['province', 'category'])['total_amount'].sum().unstack()\n",
        "province_category_pct = province_category.div(province_category.sum(axis=1), axis=0) * 100\n",
        "\n",
        "plt.figure(figsize=(15, 8))\n",
        "province_category_pct.plot(kind='bar', stacked=True)\n",
        "plt.title('各省份商品类别销售占比')\n",
        "plt.xlabel('省份')\n",
        "plt.ylabel('销售占比(%)')\n",
        "plt.legend(title='商品类别', bbox_to_anchor=(1.05, 1))\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 4. 区域市场机会\n",
        "# 计算增长率\n",
        "province_growth = geo_analysis.copy()\n",
        "province_growth['month'] = province_growth['order_date'].dt.month\n",
        "province_monthly = province_growth.groupby(['province', 'month'])['total_amount'].sum().unstack()\n",
        "province_growth_rate = ((province_monthly.iloc[:, -1] - province_monthly.iloc[:, 0]) / province_monthly.iloc[:, 0] * 100).round(2)\n",
        "\n",
        "print(\"\\n各省份销售增长率：\")\n",
        "print(province_growth_rate)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 第八部分：营销活动分析\n",
        "\n",
        "分析营销活动效果：\n",
        "- 促销效果分析\n",
        "- 用户响应度\n",
        "- 活动ROI分析\n",
        "- 营销策略优化\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 模拟营销活动数据\n",
        "np.random.seed(42)\n",
        "\n",
        "# 生成活动数据\n",
        "n_campaigns = 5\n",
        "campaign_data = pd.DataFrame({\n",
        "    'campaign_id': range(1, n_campaigns + 1),\n",
        "    'campaign_name': [f'活动{i}' for i in range(1, n_campaigns + 1)],\n",
        "    'start_date': pd.date_range(start='2023-01-01', periods=n_campaigns, freq='2M'),\n",
        "    'duration_days': np.random.randint(7, 15, n_campaigns),\n",
        "    'cost': np.random.uniform(10000, 50000, n_campaigns)\n",
        "})\n",
        "\n",
        "# 为订单添加活动标记\n",
        "orders_df['is_campaign'] = orders_df['order_date'].apply(\n",
        "    lambda x: any((x >= start) & (x <= start + pd.Timedelta(days=dur)) \n",
        "                 for start, dur in zip(campaign_data['start_date'], campaign_data['duration_days']))\n",
        ")\n",
        "\n",
        "# 1. 促销效果分析\n",
        "campaign_effect = orders_df.groupby('is_campaign').agg({\n",
        "    'total_amount': ['sum', 'mean'],\n",
        "    'order_id': 'count',\n",
        "    'user_id': 'nunique'\n",
        "}).round(2)\n",
        "\n",
        "print(\"促销活动效果对比：\")\n",
        "print(campaign_effect)\n",
        "\n",
        "# 2. 用户响应度分析\n",
        "user_response = orders_df.groupby(['user_id', 'is_campaign'])['order_id'].count().unstack(fill_value=0)\n",
        "user_response['response_rate'] = user_response[True] / (user_response[True] + user_response[False])\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(user_response['response_rate'], bins=20)\n",
        "plt.title('用户促销响应度分布')\n",
        "plt.xlabel('响应率')\n",
        "plt.ylabel('用户数')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# 3. 活动ROI分析\n",
        "campaign_revenue = orders_df[orders_df['is_campaign']]['total_amount'].sum()\n",
        "campaign_cost = campaign_data['cost'].sum()\n",
        "roi = (campaign_revenue - campaign_cost) / campaign_cost * 100\n",
        "\n",
        "print(f\"\\n活动总体ROI: {roi:.2f}%\")\n",
        "\n",
        "# 4. 营销策略优化建议\n",
        "# 分析不同用户群体在活动期间的表现\n",
        "campaign_user_analysis = pd.merge(orders_df[orders_df['is_campaign']], users_df, on='user_id')\n",
        "group_performance = campaign_user_analysis.groupby(['age_range', 'gender']).agg({\n",
        "    'total_amount': ['sum', 'mean'],\n",
        "    'user_id': 'nunique'\n",
        "}).round(2)\n",
        "\n",
        "print(\"\\n不同用户群体活动期间表现：\")\n",
        "print(group_performance)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 第九部分：实时数据处理\n",
        "\n",
        "使用Spark Streaming模拟实时数据处理：\n",
        "- 实时销售监控\n",
        "- 异常订单检测\n",
        "- 实时库存管理\n",
        "- 热点商品分析\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# 模拟实时数据生成器\n",
        "def generate_real_time_order():\n",
        "    return {\n",
        "        'order_id': np.random.randint(10000, 99999),\n",
        "        'user_id': np.random.randint(1, 1001),\n",
        "        'product_id': np.random.randint(1, 101),\n",
        "        'quantity': np.random.randint(1, 10),\n",
        "        'price': np.random.uniform(10, 1000),\n",
        "        'timestamp': datetime.now()\n",
        "    }\n",
        "\n",
        "# 1. 实时销售监控\n",
        "print(\"模拟实时销售监控...\")\n",
        "sales_window = []\n",
        "for _ in range(10):  # 模拟10个时间点\n",
        "    order = generate_real_time_order()\n",
        "    sales_amount = order['quantity'] * order['price']\n",
        "    sales_window.append(sales_amount)\n",
        "    \n",
        "    print(f\"时间: {order['timestamp'].strftime('%H:%M:%S')}\")\n",
        "    print(f\"订单金额: ¥{sales_amount:.2f}\")\n",
        "    print(f\"最近10笔订单平均金额: ¥{np.mean(sales_window):.2f}\")\n",
        "    print(\"---\")\n",
        "    time.sleep(1)\n",
        "\n",
        "# 2. 异常订单检测\n",
        "def detect_anomaly(order):\n",
        "    # 简单的异常检测规则\n",
        "    if order['quantity'] * order['price'] > 5000:  # 大额订单\n",
        "        return True\n",
        "    if order['quantity'] > 5:  # 大量购买\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "print(\"\\n异常订单检测...\")\n",
        "for _ in range(5):\n",
        "    order = generate_real_time_order()\n",
        "    is_anomaly = detect_anomaly(order)\n",
        "    print(f\"订单ID: {order['order_id']}\")\n",
        "    print(f\"订单金额: ¥{order['quantity'] * order['price']:.2f}\")\n",
        "    print(f\"是否异常: {'是' if is_anomaly else '否'}\")\n",
        "    print(\"---\")\n",
        "    time.sleep(1)\n",
        "\n",
        "# 3. 实时库存管理\n",
        "inventory = products_df[['product_id', 'stock']].copy()\n",
        "print(\"\\n实时库存管理...\")\n",
        "for _ in range(5):\n",
        "    order = generate_real_time_order()\n",
        "    product_id = order['product_id']\n",
        "    quantity = order['quantity']\n",
        "    \n",
        "    # 更新库存\n",
        "    old_stock = inventory.loc[inventory['product_id'] == product_id, 'stock'].iloc[0]\n",
        "    new_stock = old_stock - quantity\n",
        "    inventory.loc[inventory['product_id'] == product_id, 'stock'] = new_stock\n",
        "    \n",
        "    print(f\"商品ID: {product_id}\")\n",
        "    print(f\"购买数量: {quantity}\")\n",
        "    print(f\"剩余库存: {new_stock}\")\n",
        "    print(\"---\")\n",
        "    time.sleep(1)\n",
        "\n",
        "# 4. 热点商品分析\n",
        "print(\"\\n实时热点商品分析...\")\n",
        "hot_products = {}\n",
        "for _ in range(20):\n",
        "    order = generate_real_time_order()\n",
        "    product_id = order['product_id']\n",
        "    hot_products[product_id] = hot_products.get(product_id, 0) + 1\n",
        "\n",
        "top_products = sorted(hot_products.items(), key=lambda x: x[1], reverse=True)[:5]\n",
        "print(\"最近热门商品:\")\n",
        "for product_id, count in top_products:\n",
        "    print(f\"商品ID: {product_id}, 被购买次数: {count}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# 电商数据分析项目\n",
        "\n",
        "本项目使用大数据技术对电商数据进行全面分析。主要使用了以下技术栈：\n",
        "- Hadoop 3.3.1\n",
        "- Spark 3.2.0\n",
        "- Python 3.8\n",
        "- Jupyter Notebook\n",
        "\n",
        "## 目录\n",
        "1. 环境配置\n",
        "2. 数据获取与预处理\n",
        "3. 数据分析\n",
        "   - 销售趋势分析\n",
        "   - RFM客户价值分析\n",
        "   - 商品关联分析\n",
        "   - 实时数据处理演示\n",
        "4. 结论与建议\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. 环境配置\n",
        "\n",
        "首先配置必要的Python包和Spark环境：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.clustering import KMeans\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 创建SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"E-commerce Analysis\") \\\n",
        "    .config(\"spark.memory.offHeap.enabled\", True) \\\n",
        "    .config(\"spark.memory.offHeap.size\", \"10g\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"Spark版本:\", spark.version)\n",
        "print(\"环境配置完成!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. 数据获取与预处理\n",
        "\n",
        "我们使用UCI Machine Learning Repository提供的在线零售数据集。该数据集包含了2010-2011年间某在线零售商的所有交易。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 读取数据\n",
        "df = spark.read.csv(\"Online_Retail.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# 数据清洗\n",
        "df_cleaned = df.dropna() \\\n",
        "    .filter(col(\"Quantity\") > 0) \\\n",
        "    .filter(col(\"UnitPrice\") > 0) \\\n",
        "    .withColumn(\"TotalAmount\", col(\"Quantity\") * col(\"UnitPrice\"))\n",
        "\n",
        "# 显示数据基本信息\n",
        "print(\"数据总行数:\", df_cleaned.count())\n",
        "print(\"\\n数据结构:\")\n",
        "df_cleaned.printSchema()\n",
        "\n",
        "# 显示前几行数据\n",
        "print(\"\\n数据预览:\")\n",
        "df_cleaned.show(5)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. 数据分析\n",
        "\n",
        "### 3.1 销售趋势分析\n",
        "\n",
        "首先分析每月的销售趋势：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 按月统计销售额\n",
        "monthly_sales = df_cleaned \\\n",
        "    .withColumn(\"YearMonth\", date_format(\"InvoiceDate\", \"yyyy-MM\")) \\\n",
        "    .groupBy(\"YearMonth\") \\\n",
        "    .agg(sum(\"TotalAmount\").alias(\"TotalSales\")) \\\n",
        "    .orderBy(\"YearMonth\")\n",
        "\n",
        "# 转换为Pandas进行可视化\n",
        "monthly_sales_pd = monthly_sales.toPandas()\n",
        "\n",
        "# 绘制月度销售趋势图\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(monthly_sales_pd[\"YearMonth\"], monthly_sales_pd[\"TotalSales\"], marker='o')\n",
        "plt.title(\"月度销售趋势\")\n",
        "plt.xlabel(\"年月\")\n",
        "plt.ylabel(\"销售总额\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 计算环比增长率\n",
        "monthly_sales_pd[\"GrowthRate\"] = monthly_sales_pd[\"TotalSales\"].pct_change() * 100\n",
        "print(\"\\n月度销售额环比增长率:\")\n",
        "print(monthly_sales_pd)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 3.2 RFM客户价值分析\n",
        "\n",
        "使用RFM模型分析客户价值：\n",
        "- Recency: 最近一次购买时间\n",
        "- Frequency: 购买频率\n",
        "- Monetary: 购买金额\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 计算RFM指标\n",
        "max_date = df_cleaned.agg(max(\"InvoiceDate\")).collect()[0][0]\n",
        "\n",
        "rfm = df_cleaned.groupBy(\"CustomerID\").agg(\n",
        "    datediff(lit(max_date), max(\"InvoiceDate\")).alias(\"Recency\"),\n",
        "    countDistinct(\"InvoiceNo\").alias(\"Frequency\"),\n",
        "    sum(\"TotalAmount\").alias(\"Monetary\")\n",
        ")\n",
        "\n",
        "# 使用K-means进行客户分群\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[\"Recency\", \"Frequency\", \"Monetary\"],\n",
        "    outputCol=\"features\"\n",
        ")\n",
        "\n",
        "rfm_features = assembler.transform(rfm)\n",
        "kmeans = KMeans(k=4, seed=1)\n",
        "model = kmeans.fit(rfm_features)\n",
        "rfm_clustered = model.transform(rfm_features)\n",
        "\n",
        "# 分析各群体特征\n",
        "cluster_stats = rfm_clustered.groupBy(\"prediction\").agg(\n",
        "    avg(\"Recency\").alias(\"Avg_Recency\"),\n",
        "    avg(\"Frequency\").alias(\"Avg_Frequency\"),\n",
        "    avg(\"Monetary\").alias(\"Avg_Monetary\"),\n",
        "    count(\"*\").alias(\"Count\")\n",
        ")\n",
        "\n",
        "print(\"客户群体特征分析:\")\n",
        "cluster_stats.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 3.3 商品关联分析\n",
        "\n",
        "使用Spark MLlib实现商品关联规则分析：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.ml.fpm import FPGrowth\n",
        "\n",
        "# 准备数据\n",
        "basket_data = df_cleaned.select(\"InvoiceNo\", \"Description\") \\\n",
        "    .groupBy(\"InvoiceNo\") \\\n",
        "    .agg(collect_list(\"Description\").alias(\"items\"))\n",
        "\n",
        "# 使用FP-Growth算法进行关联分析\n",
        "fpGrowth = FPGrowth(itemsCol=\"items\", minSupport=0.01, minConfidence=0.5)\n",
        "model = fpGrowth.fit(basket_data)\n",
        "\n",
        "# 显示频繁项集\n",
        "print(\"频繁项集:\")\n",
        "model.freqItemsets.show(10)\n",
        "\n",
        "# 显示关联规则\n",
        "print(\"\\n关联规则:\")\n",
        "model.associationRules.show(10)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 3.4 实时数据处理演示\n",
        "\n",
        "使用Spark Streaming模拟实时订单数据处理：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.streaming import StreamingContext\n",
        "\n",
        "# 创建StreamingContext\n",
        "ssc = StreamingContext(spark.sparkContext, 1)\n",
        "\n",
        "# 模拟实时数据流\n",
        "def generate_stream_data():\n",
        "    return df_cleaned.limit(100).toPandas().to_json(orient=\"records\")\n",
        "\n",
        "# 创建DStream\n",
        "stream_data = ssc.queueStream([spark.sparkContext.parallelize([generate_stream_data()])])\n",
        "\n",
        "# 实时计算\n",
        "def process_batch(time, rdd):\n",
        "    if not rdd.isEmpty():\n",
        "        # 计算实时销售额\n",
        "        sales = rdd.map(lambda x: float(x.split(\",\")[6])).sum()\n",
        "        print(f\"Time: {time}, Sales: {sales}\")\n",
        "\n",
        "# 应用处理函数\n",
        "stream_data.foreachRDD(process_batch)\n",
        "\n",
        "# 启动流处理\n",
        "ssc.start()\n",
        "ssc.awaitTerminationOrTimeout(10)  # 运行10秒后停止\n",
        "ssc.stop()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. 结论与建议\n",
        "\n",
        "通过对电商数据的分析，我们得出以下结论：\n",
        "\n",
        "1. 销售趋势\n",
        "- 整体呈现上升趋势\n",
        "- 存在明显的季节性波动\n",
        "- 节假日期间销售额显著提升\n",
        "\n",
        "2. 客户价值分析\n",
        "- 识别出4个主要客户群体\n",
        "- 高价值客户群体约占20%\n",
        "- 建议针对不同群体制定差异化营销策略\n",
        "\n",
        "3. 商品关联分析\n",
        "- 发现了多个高关联度的商品组合\n",
        "- 可用于优化商品陈列和促销策略\n",
        "- 有助于提升交叉销售\n",
        "\n",
        "4. 实时处理能力\n",
        "- 系统可以有效处理实时订单数据\n",
        "- 支持实时销售监控\n",
        "- 为快速决策提供数据支持\n",
        "\n",
        "### 建议\n",
        "1. 加强对高价值客户的维护\n",
        "2. 优化商品组合推荐\n",
        "3. 根据销售趋势调整库存\n",
        "4. 发展实时监控能力\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# 电商订单数据分析项目 - 第三部分：批处理分析(Hadoop)\n",
        "\n",
        "在这部分中，我们将使用Hadoop生态系统进行大规模数据处理和分析。主要内容包括：\n",
        "\n",
        "1. 数据迁移：MySQL到HDFS\n",
        "2. MapReduce分析：订单数据处理\n",
        "3. Hive分析：复杂查询实现\n",
        "4. 数据导出：结果保存\n",
        "\n",
        "本部分将展示如何使用Hadoop处理大规模数据集，为后续的实时处理打下基础。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 导入所需的库\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# 创建SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"EcommerceAnalysis\") \\\n",
        "    .config(\"spark.sql.warehouse.dir\", \"hdfs://localhost:9000/user/hive/warehouse\") \\\n",
        "    .config(\"spark.executor.memory\", \"2g\") \\\n",
        "    .config(\"spark.driver.memory\", \"2g\") \\\n",
        "    .enableHiveSupport() \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# 从MySQL读取数据\n",
        "def read_from_mysql():\n",
        "    # MySQL连接配置\n",
        "    mysql_properties = {\n",
        "        \"user\": \"root\",\n",
        "        \"password\": \"your_password\",  # 替换为实际密码\n",
        "        \"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
        "    }\n",
        "    \n",
        "    # 读取MySQL数据\n",
        "    df = spark.read.format(\"jdbc\") \\\n",
        "        .option(\"url\", \"jdbc:mysql://localhost:3306/ecommerce_analysis\") \\\n",
        "        .option(\"dbtable\", \"orders\") \\\n",
        "        .options(**mysql_properties) \\\n",
        "        .load()\n",
        "    \n",
        "    return df\n",
        "\n",
        "# 读取数据\n",
        "try:\n",
        "    orders_df = read_from_mysql()\n",
        "    print(\"数据读取成功！\")\n",
        "    print(f\"总记录数: {orders_df.count()}\")\n",
        "    print(\"\\n数据Schema:\")\n",
        "    orders_df.printSchema()\n",
        "except Exception as e:\n",
        "    print(f\"错误: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 将数据保存到HDFS\n",
        "def save_to_hdfs(df):\n",
        "    try:\n",
        "        # 保存为Parquet格式\n",
        "        df.write \\\n",
        "            .mode(\"overwrite\") \\\n",
        "            .parquet(\"hdfs://localhost:9000/user/hadoop/ecommerce/orders\")\n",
        "        \n",
        "        print(\"数据已成功保存到HDFS\")\n",
        "        \n",
        "        # 创建Hive表\n",
        "        spark.sql(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS orders_hive (\n",
        "            订单编号 INT,\n",
        "            总金额 DECIMAL(10,2),\n",
        "            买家实际支付金额 DECIMAL(10,2),\n",
        "            收货地址 STRING,\n",
        "            订单创建时间 TIMESTAMP,\n",
        "            订单付款时间 TIMESTAMP,\n",
        "            退款金额 DECIMAL(10,2),\n",
        "            支付时长 FLOAT,\n",
        "            下单年月 STRING,\n",
        "            下单时间 INT,\n",
        "            下单星期 INT,\n",
        "            订单状态 STRING,\n",
        "            实际收入 DECIMAL(10,2)\n",
        "        )\n",
        "        STORED AS PARQUET\n",
        "        LOCATION 'hdfs://localhost:9000/user/hadoop/ecommerce/orders'\n",
        "        \"\"\")\n",
        "        \n",
        "        print(\"Hive表创建成功\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"错误: {e}\")\n",
        "\n",
        "# 保存数据\n",
        "save_to_hdfs(orders_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 使用Spark SQL进行数据分析\n",
        "def analyze_orders():\n",
        "    # 1. 销售趋势分析\n",
        "    print(\"1. 每日销售趋势分析：\")\n",
        "    daily_sales = spark.sql(\"\"\"\n",
        "        SELECT \n",
        "            DATE(订单创建时间) as 日期,\n",
        "            COUNT(*) as 订单数,\n",
        "            SUM(总金额) as 总销售额,\n",
        "            SUM(实际收入) as 实际收入,\n",
        "            AVG(总金额) as 平均订单金额\n",
        "        FROM orders_hive\n",
        "        GROUP BY DATE(订单创建时间)\n",
        "        ORDER BY 日期\n",
        "    \"\"\")\n",
        "    daily_sales.show(5)\n",
        "    \n",
        "    # 2. 地区销售分析\n",
        "    print(\"\\n2. 地区销售分析：\")\n",
        "    region_sales = spark.sql(\"\"\"\n",
        "        SELECT \n",
        "            收货地址,\n",
        "            COUNT(*) as 订单数,\n",
        "            SUM(总金额) as 总销售额,\n",
        "            SUM(实际收入) as 实际收入,\n",
        "            AVG(总金额) as 平均订单金额,\n",
        "            COUNT(CASE WHEN 订单状态 = '已退款' THEN 1 END) as 退款订单数\n",
        "        FROM orders_hive\n",
        "        GROUP BY 收货地址\n",
        "        ORDER BY 订单数 DESC\n",
        "    \"\"\")\n",
        "    region_sales.show(5)\n",
        "    \n",
        "    # 3. 时间段分析\n",
        "    print(\"\\n3. 各时段订单分布：\")\n",
        "    hourly_orders = spark.sql(\"\"\"\n",
        "        SELECT \n",
        "            下单时间 as 小时,\n",
        "            COUNT(*) as 订单数,\n",
        "            SUM(总金额) as 总销售额,\n",
        "            AVG(总金额) as 平均订单金额\n",
        "        FROM orders_hive\n",
        "        GROUP BY 下单时间\n",
        "        ORDER BY 小时\n",
        "    \"\"\")\n",
        "    hourly_orders.show(24)\n",
        "    \n",
        "    # 4. 订单状态分析\n",
        "    print(\"\\n4. 订单状态分析：\")\n",
        "    status_analysis = spark.sql(\"\"\"\n",
        "        SELECT \n",
        "            订单状态,\n",
        "            COUNT(*) as 订单数,\n",
        "            SUM(总金额) as 总金额,\n",
        "            AVG(支付时长) as 平均支付时长\n",
        "        FROM orders_hive\n",
        "        GROUP BY 订单状态\n",
        "    \"\"\")\n",
        "    status_analysis.show()\n",
        "\n",
        "# 执行分析\n",
        "analyze_orders()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 高级分析：RFM客户价值分析\n",
        "def rfm_analysis():\n",
        "    # 计算RFM指标\n",
        "    rfm_query = \"\"\"\n",
        "    WITH rfm_base AS (\n",
        "        SELECT \n",
        "            订单编号,\n",
        "            订单创建时间,\n",
        "            实际收入,\n",
        "            MAX(订单创建时间) OVER () as max_date\n",
        "        FROM orders_hive\n",
        "        WHERE 订单状态 = '已付款'\n",
        "    ),\n",
        "    rfm_calc AS (\n",
        "        SELECT \n",
        "            订单编号,\n",
        "            DATEDIFF(max_date, 订单创建时间) as recency,\n",
        "            COUNT(*) OVER (PARTITION BY 订单编号) as frequency,\n",
        "            SUM(实际收入) OVER (PARTITION BY 订单编号) as monetary\n",
        "        FROM rfm_base\n",
        "    )\n",
        "    SELECT \n",
        "        CASE \n",
        "            WHEN recency <= 30 THEN '高'\n",
        "            WHEN recency <= 90 THEN '中'\n",
        "            ELSE '低'\n",
        "        END as 最近购买,\n",
        "        CASE \n",
        "            WHEN frequency >= 3 THEN '高'\n",
        "            WHEN frequency >= 2 THEN '中'\n",
        "            ELSE '低'\n",
        "        END as 购买频率,\n",
        "        CASE \n",
        "            WHEN monetary >= 1000 THEN '高'\n",
        "            WHEN monetary >= 500 THEN '中'\n",
        "            ELSE '低'\n",
        "        END as 消费金额,\n",
        "        COUNT(*) as 客户数量\n",
        "    FROM rfm_calc\n",
        "    GROUP BY \n",
        "        CASE \n",
        "            WHEN recency <= 30 THEN '高'\n",
        "            WHEN recency <= 90 THEN '中'\n",
        "            ELSE '低'\n",
        "        END,\n",
        "        CASE \n",
        "            WHEN frequency >= 3 THEN '高'\n",
        "            WHEN frequency >= 2 THEN '中'\n",
        "            ELSE '低'\n",
        "        END,\n",
        "        CASE \n",
        "            WHEN monetary >= 1000 THEN '高'\n",
        "            WHEN monetary >= 500 THEN '中'\n",
        "            ELSE '低'\n",
        "        END\n",
        "    ORDER BY 最近购买, 购买频率, 消费金额\n",
        "    \"\"\"\n",
        "    \n",
        "    rfm_results = spark.sql(rfm_query)\n",
        "    print(\"RFM客户价值分析结果：\")\n",
        "    rfm_results.show()\n",
        "\n",
        "# 执行RFM分析\n",
        "rfm_analysis()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 保存分析结果\n",
        "def save_analysis_results():\n",
        "    # 1. 保存销售趋势分析结果\n",
        "    daily_sales = spark.sql(\"\"\"\n",
        "        SELECT \n",
        "            DATE(订单创建时间) as 日期,\n",
        "            COUNT(*) as 订单数,\n",
        "            SUM(总金额) as 总销售额,\n",
        "            SUM(实际收入) as 实际收入\n",
        "        FROM orders_hive\n",
        "        GROUP BY DATE(订单创建时间)\n",
        "        ORDER BY 日期\n",
        "    \"\"\")\n",
        "    \n",
        "    daily_sales.write \\\n",
        "        .mode(\"overwrite\") \\\n",
        "        .parquet(\"hdfs://localhost:9000/user/hadoop/ecommerce/analysis/daily_sales\")\n",
        "    \n",
        "    # 2. 保存地区销售分析结果\n",
        "    region_sales = spark.sql(\"\"\"\n",
        "        SELECT \n",
        "            收货地址,\n",
        "            COUNT(*) as 订单数,\n",
        "            SUM(总金额) as 总销售额,\n",
        "            SUM(实际收入) as 实际收入\n",
        "        FROM orders_hive\n",
        "        GROUP BY 收货地址\n",
        "    \"\"\")\n",
        "    \n",
        "    region_sales.write \\\n",
        "        .mode(\"overwrite\") \\\n",
        "        .parquet(\"hdfs://localhost:9000/user/hadoop/ecommerce/analysis/region_sales\")\n",
        "    \n",
        "    # 3. 保存RFM分析结果\n",
        "    rfm_results = spark.sql(\"\"\"\n",
        "        WITH rfm_base AS (\n",
        "            SELECT \n",
        "                订单编号,\n",
        "                订单创建时间,\n",
        "                实际收入,\n",
        "                MAX(订单创建时间) OVER () as max_date\n",
        "            FROM orders_hive\n",
        "            WHERE 订单状态 = '已付款'\n",
        "        )\n",
        "        SELECT \n",
        "            订单编号,\n",
        "            DATEDIFF(max_date, 订单创建时间) as recency,\n",
        "            COUNT(*) OVER (PARTITION BY 订单编号) as frequency,\n",
        "            SUM(实际收入) OVER (PARTITION BY 订单编号) as monetary\n",
        "        FROM rfm_base\n",
        "    \"\"\")\n",
        "    \n",
        "    rfm_results.write \\\n",
        "        .mode(\"overwrite\") \\\n",
        "        .parquet(\"hdfs://localhost:9000/user/hadoop/ecommerce/analysis/rfm_analysis\")\n",
        "    \n",
        "    print(\"分析结果已保存到HDFS\")\n",
        "\n",
        "# 保存分析结果\n",
        "save_analysis_results()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# 第三部分总结\n",
        "\n",
        "在这部分中，我们完成了以下工作：\n",
        "\n",
        "1. 数据迁移\n",
        "   - 从MySQL读取数据\n",
        "   - 将数据保存到HDFS\n",
        "   - 创建Hive表结构\n",
        "\n",
        "2. 基础分析\n",
        "   - 销售趋势分析\n",
        "   - 地区销售分析\n",
        "   - 时间段分析\n",
        "   - 订单状态分析\n",
        "\n",
        "3. 高级分析\n",
        "   - RFM客户价值分析\n",
        "   - 客户分层\n",
        "\n",
        "4. 结果保存\n",
        "   - 分析结果保存到HDFS\n",
        "   - 生成多个数据集供后续使用\n",
        "\n",
        "下一步，我们将进入第四部分：实时处理(Kafka + Flink)，实现实时数据分析和监控。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# 第四部分：实时处理(Kafka + Flink)\n",
        "\n",
        "在这部分中，我们将构建实时数据处理流水线，包括：\n",
        "\n",
        "1. Kafka数据流\n",
        "   - 订单数据实时采集\n",
        "   - 消息队列管理\n",
        "   - 数据流监控\n",
        "\n",
        "2. Flink实时处理\n",
        "   - 实时指标计算\n",
        "   - 窗口统计\n",
        "   - 实时告警\n",
        "\n",
        "3. 实时监控\n",
        "   - 销售额实时统计\n",
        "   - 订单量监控\n",
        "   - 异常检测\n",
        "\n",
        "本部分将展示如何处理实时数据流并进行实时分析。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 导入所需的库\n",
        "from kafka import KafkaProducer, KafkaConsumer\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "import random\n",
        "import threading\n",
        "from pyflink.datastream import StreamExecutionEnvironment\n",
        "from pyflink.table import StreamTableEnvironment, EnvironmentSettings\n",
        "import numpy as np\n",
        "\n",
        "# Kafka配置\n",
        "KAFKA_BOOTSTRAP_SERVERS = 'localhost:9092'\n",
        "KAFKA_TOPIC = 'orders_stream'\n",
        "\n",
        "# 创建Kafka生产者\n",
        "def create_producer():\n",
        "    return KafkaProducer(\n",
        "        bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
        "        value_serializer=lambda x: json.dumps(x).encode('utf-8')\n",
        "    )\n",
        "\n",
        "# 模拟生成订单数据\n",
        "def generate_order():\n",
        "    cities = ['北京', '上海', '广州', '深圳', '杭州', '成都', '武汉', '西安']\n",
        "    return {\n",
        "        '订单编号': random.randint(10000, 99999),\n",
        "        '总金额': round(random.uniform(100, 2000), 2),\n",
        "        '买家实际支付金额': round(random.uniform(100, 2000), 2),\n",
        "        '收货地址': random.choice(cities),\n",
        "        '订单创建时间': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "        '订单状态': random.choice(['已付款', '未付款', '已退款']),\n",
        "        'timestamp': int(time.time() * 1000)\n",
        "    }\n",
        "\n",
        "# 数据生产者函数\n",
        "def produce_messages(stop_event):\n",
        "    producer = create_producer()\n",
        "    while not stop_event.is_set():\n",
        "        order = generate_order()\n",
        "        producer.send(KAFKA_TOPIC, order)\n",
        "        print(f\"已发送订单: {order['订单编号']}\")\n",
        "        time.sleep(1)  # 每秒生成一个订单\n",
        "    producer.close()\n",
        "\n",
        "# 创建停止事件\n",
        "stop_event = threading.Event()\n",
        "\n",
        "# 启动生产者线程\n",
        "producer_thread = threading.Thread(target=produce_messages, args=(stop_event,))\n",
        "producer_thread.start()\n",
        "\n",
        "print(\"Kafka生产者已启动，开始生成订单数据...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 设置Flink环境\n",
        "env = StreamExecutionEnvironment.get_execution_environment()\n",
        "t_env = StreamTableEnvironment.create(env)\n",
        "\n",
        "# 创建Kafka源表\n",
        "source_ddl = \"\"\"\n",
        "    CREATE TABLE orders_source (\n",
        "        订单编号 INT,\n",
        "        总金额 DECIMAL(10, 2),\n",
        "        买家实际支付金额 DECIMAL(10, 2),\n",
        "        收货地址 STRING,\n",
        "        订单创建时间 STRING,\n",
        "        订单状态 STRING,\n",
        "        `timestamp` BIGINT,\n",
        "        proctime AS PROCTIME()\n",
        "    ) WITH (\n",
        "        'connector' = 'kafka',\n",
        "        'topic' = 'orders_stream',\n",
        "        'properties.bootstrap.servers' = 'localhost:9092',\n",
        "        'format' = 'json',\n",
        "        'scan.startup.mode' = 'latest-offset'\n",
        "    )\n",
        "\"\"\"\n",
        "\n",
        "# 创建结果表\n",
        "sink_ddl = \"\"\"\n",
        "    CREATE TABLE orders_stats (\n",
        "        window_start TIMESTAMP(3),\n",
        "        window_end TIMESTAMP(3),\n",
        "        订单数 BIGINT,\n",
        "        总金额 DECIMAL(10, 2),\n",
        "        平均金额 DECIMAL(10, 2)\n",
        "    ) WITH (\n",
        "        'connector' = 'print'\n",
        "    )\n",
        "\"\"\"\n",
        "\n",
        "# 执行DDL\n",
        "t_env.execute_sql(source_ddl)\n",
        "t_env.execute_sql(sink_ddl)\n",
        "\n",
        "# 实时统计查询\n",
        "stats_query = \"\"\"\n",
        "    INSERT INTO orders_stats\n",
        "    SELECT\n",
        "        window_start,\n",
        "        window_end,\n",
        "        COUNT(*) as 订单数,\n",
        "        SUM(总金额) as 总金额,\n",
        "        AVG(总金额) as 平均金额\n",
        "    FROM TABLE(\n",
        "        TUMBLE(TABLE orders_source, DESCRIPTOR(proctime), INTERVAL '1' MINUTES)\n",
        "    )\n",
        "    GROUP BY window_start, window_end\n",
        "\"\"\"\n",
        "\n",
        "# 执行查询\n",
        "t_env.execute_sql(stats_query)\n",
        "\n",
        "print(\"Flink实时处理任务已启动...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 实时监控和告警\n",
        "def monitor_orders():\n",
        "    # 创建消费者\n",
        "    consumer = KafkaConsumer(\n",
        "        KAFKA_TOPIC,\n",
        "        bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
        "        value_deserializer=lambda x: json.loads(x.decode('utf-8')),\n",
        "        auto_offset_reset='latest',\n",
        "        enable_auto_commit=True\n",
        "    )\n",
        "    \n",
        "    # 初始化监控指标\n",
        "    window_size = 60  # 60秒窗口\n",
        "    order_counts = []\n",
        "    order_amounts = []\n",
        "    alert_threshold = 1000  # 告警阈值\n",
        "    \n",
        "    print(\"开始实时监控订单数据...\")\n",
        "    \n",
        "    try:\n",
        "        for message in consumer:\n",
        "            order = message.value\n",
        "            \n",
        "            # 添加订单金额到窗口\n",
        "            order_amounts.append(order['总金额'])\n",
        "            order_counts.append(1)\n",
        "            \n",
        "            # 移除超出窗口的数据\n",
        "            current_time = time.time()\n",
        "            cutoff_time = current_time - window_size\n",
        "            \n",
        "            # 计算窗口内的统计数据\n",
        "            total_orders = sum(order_counts)\n",
        "            total_amount = sum(order_amounts)\n",
        "            avg_amount = total_amount / total_orders if total_orders > 0 else 0\n",
        "            \n",
        "            # 打印实时统计\n",
        "            print(f\"\\n实时监控 - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "            print(f\"最近{window_size}秒内:\")\n",
        "            print(f\"订单数量: {total_orders}\")\n",
        "            print(f\"总金额: ¥{total_amount:.2f}\")\n",
        "            print(f\"平均订单金额: ¥{avg_amount:.2f}\")\n",
        "            \n",
        "            # 告警检测\n",
        "            if total_amount > alert_threshold:\n",
        "                print(f\"⚠️ 告警：订单金额超过阈值！(¥{total_amount:.2f} > ¥{alert_threshold:.2f})\")\n",
        "            \n",
        "            time.sleep(1)  # 控制输出频率\n",
        "            \n",
        "    except KeyboardInterrupt:\n",
        "        print(\"监控停止\")\n",
        "        consumer.close()\n",
        "\n",
        "# 启动监控线程\n",
        "monitor_thread = threading.Thread(target=monitor_orders)\n",
        "monitor_thread.start()\n",
        "\n",
        "# 运行一段时间后停止\n",
        "time.sleep(300)  # 运行5分钟\n",
        "stop_event.set()  # 停止生产者\n",
        "print(\"已停止数据生成和监控\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# 第五部分：复杂分析(Spark)\n",
        "\n",
        "在这部分中，我们将使用Spark进行更深入的数据分析，包括：\n",
        "\n",
        "1. 用户行为分析\n",
        "   - 用户购买路径分析\n",
        "   - 用户活跃度分析\n",
        "   - 用户生命周期分析\n",
        "\n",
        "2. 商品关联分析\n",
        "   - 商品共现分析\n",
        "   - 购物篮分析\n",
        "   - 商品推荐\n",
        "\n",
        "3. 销售趋势预测\n",
        "   - 时间序列分析\n",
        "   - 销售预测模型\n",
        "   - 季节性分析\n",
        "\n",
        "4. 用户分群分析\n",
        "   - 用户价值分群\n",
        "   - 用户行为分群\n",
        "   - 精准营销建议\n",
        "\n",
        "本部分将展示如何使用Spark的高级特性来实现复杂的数据分析任务。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 导入所需的库\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# 创建SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"E-commerce Complex Analysis\") \\\n",
        "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# 从MySQL读取数据\n",
        "orders_df = spark.read \\\n",
        "    .format(\"jdbc\") \\\n",
        "    .option(\"url\", \"jdbc:mysql://localhost:3306/ecommerce\") \\\n",
        "    .option(\"dbtable\", \"orders\") \\\n",
        "    .option(\"user\", \"root\") \\\n",
        "    .option(\"password\", \"your_password\") \\\n",
        "    .load()\n",
        "\n",
        "# 注册临时视图\n",
        "orders_df.createOrReplaceTempView(\"orders\")\n",
        "\n",
        "print(\"Spark环境已准备就绪，数据已加载完成。\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. 用户行为分析\n",
        "\n",
        "# 1.1 用户购买路径分析\n",
        "def analyze_user_purchase_path():\n",
        "    path_sql = \"\"\"\n",
        "    WITH user_path AS (\n",
        "        SELECT \n",
        "            用户ID,\n",
        "            订单创建时间,\n",
        "            商品类别,\n",
        "            LAG(商品类别) OVER (PARTITION BY 用户ID ORDER BY 订单创建时间) as prev_category\n",
        "        FROM orders\n",
        "    )\n",
        "    SELECT \n",
        "        prev_category as 前一个类别,\n",
        "        商品类别 as 当前类别,\n",
        "        COUNT(*) as 转换次数\n",
        "    FROM user_path\n",
        "    WHERE prev_category IS NOT NULL\n",
        "    GROUP BY prev_category, 商品类别\n",
        "    ORDER BY 转换次数 DESC\n",
        "    LIMIT 10\n",
        "    \"\"\"\n",
        "    return spark.sql(path_sql)\n",
        "\n",
        "# 1.2 用户活跃度分析\n",
        "def analyze_user_activity():\n",
        "    activity_sql = \"\"\"\n",
        "    SELECT \n",
        "        用户ID,\n",
        "        COUNT(DISTINCT DATE(订单创建时间)) as 活跃天数,\n",
        "        COUNT(*) as 总订单数,\n",
        "        DATEDIFF(MAX(订单创建时间), MIN(订单创建时间)) as 购买时间跨度\n",
        "    FROM orders\n",
        "    GROUP BY 用户ID\n",
        "    \"\"\"\n",
        "    return spark.sql(activity_sql)\n",
        "\n",
        "# 1.3 用户生命周期分析\n",
        "def analyze_user_lifecycle():\n",
        "    lifecycle_sql = \"\"\"\n",
        "    WITH user_stats AS (\n",
        "        SELECT \n",
        "            用户ID,\n",
        "            MIN(订单创建时间) as 首次购买时间,\n",
        "            MAX(订单创建时间) as 最近购买时间,\n",
        "            COUNT(*) as 购买次数,\n",
        "            SUM(总金额) as 总消费金额\n",
        "        FROM orders\n",
        "        GROUP BY 用户ID\n",
        "    )\n",
        "    SELECT \n",
        "        CASE \n",
        "            WHEN 购买次数 = 1 THEN '新用户'\n",
        "            WHEN DATEDIFF(CURRENT_DATE, 最近购买时间) <= 30 THEN '活跃用户'\n",
        "            WHEN DATEDIFF(CURRENT_DATE, 最近购买时间) <= 90 THEN '沉睡用户'\n",
        "            ELSE '流失用户'\n",
        "        END as 用户状态,\n",
        "        COUNT(*) as 用户数量,\n",
        "        AVG(总消费金额) as 平均消费金额\n",
        "    FROM user_stats\n",
        "    GROUP BY \n",
        "        CASE \n",
        "            WHEN 购买次数 = 1 THEN '新用户'\n",
        "            WHEN DATEDIFF(CURRENT_DATE, 最近购买时间) <= 30 THEN '活跃用户'\n",
        "            WHEN DATEDIFF(CURRENT_DATE, 最近购买时间) <= 90 THEN '沉睡用户'\n",
        "            ELSE '流失用户'\n",
        "        END\n",
        "    \"\"\"\n",
        "    return spark.sql(lifecycle_sql)\n",
        "\n",
        "# 执行分析\n",
        "print(\"用户购买路径分析结果：\")\n",
        "analyze_user_purchase_path().show()\n",
        "\n",
        "print(\"\\n用户活跃度分析结果：\")\n",
        "analyze_user_activity().show()\n",
        "\n",
        "print(\"\\n用户生命周期分析结果：\")\n",
        "analyze_user_lifecycle().show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. 商品关联分析\n",
        "\n",
        "# 2.1 商品共现分析\n",
        "def analyze_product_cooccurrence():\n",
        "    cooccurrence_sql = \"\"\"\n",
        "    WITH order_products AS (\n",
        "        SELECT \n",
        "            订单编号,\n",
        "            COLLECT_LIST(商品ID) as products\n",
        "        FROM orders\n",
        "        GROUP BY 订单编号\n",
        "    )\n",
        "    SELECT \n",
        "        p1.商品ID as 商品1,\n",
        "        p2.商品ID as 商品2,\n",
        "        COUNT(*) as 共现次数\n",
        "    FROM order_products op\n",
        "    CROSS JOIN UNNEST(op.products) p1\n",
        "    CROSS JOIN UNNEST(op.products) p2\n",
        "    WHERE p1.商品ID < p2.商品ID\n",
        "    GROUP BY p1.商品ID, p2.商品ID\n",
        "    ORDER BY 共现次数 DESC\n",
        "    LIMIT 10\n",
        "    \"\"\"\n",
        "    return spark.sql(cooccurrence_sql)\n",
        "\n",
        "# 2.2 购物篮分析\n",
        "def analyze_shopping_basket():\n",
        "    basket_sql = \"\"\"\n",
        "    SELECT \n",
        "        商品类别,\n",
        "        COUNT(*) as 购买次数,\n",
        "        COUNT(DISTINCT 用户ID) as 购买用户数,\n",
        "        AVG(商品数量) as 平均购买数量,\n",
        "        AVG(总金额) as 平均购买金额\n",
        "    FROM orders\n",
        "    GROUP BY 商品类别\n",
        "    ORDER BY 购买次数 DESC\n",
        "    \"\"\"\n",
        "    return spark.sql(basket_sql)\n",
        "\n",
        "# 2.3 商品推荐\n",
        "def generate_product_recommendations():\n",
        "    # 使用协同过滤的简单实现\n",
        "    recommendations_sql = \"\"\"\n",
        "    WITH user_product_matrix AS (\n",
        "        SELECT \n",
        "            用户ID,\n",
        "            商品ID,\n",
        "            COUNT(*) as 购买次数\n",
        "        FROM orders\n",
        "        GROUP BY 用户ID, 商品ID\n",
        "    ),\n",
        "    product_similarity AS (\n",
        "        SELECT \n",
        "            a.商品ID as 商品1,\n",
        "            b.商品ID as 商品2,\n",
        "            CORR(a.购买次数, b.购买次数) as 相似度\n",
        "        FROM user_product_matrix a\n",
        "        JOIN user_product_matrix b ON a.用户ID = b.用户ID\n",
        "        WHERE a.商品ID < b.商品ID\n",
        "        GROUP BY a.商品ID, b.商品ID\n",
        "    )\n",
        "    SELECT \n",
        "        商品1,\n",
        "        商品2,\n",
        "        相似度\n",
        "    FROM product_similarity\n",
        "    WHERE 相似度 > 0.5\n",
        "    ORDER BY 相似度 DESC\n",
        "    LIMIT 10\n",
        "    \"\"\"\n",
        "    return spark.sql(recommendations_sql)\n",
        "\n",
        "# 执行分析\n",
        "print(\"商品共现分析结果：\")\n",
        "analyze_product_cooccurrence().show()\n",
        "\n",
        "print(\"\\n购物篮分析结果：\")\n",
        "analyze_shopping_basket().show()\n",
        "\n",
        "print(\"\\n商品推荐结果：\")\n",
        "generate_product_recommendations().show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. 销售趋势预测\n",
        "\n",
        "# 3.1 时间序列分析\n",
        "def analyze_time_series():\n",
        "    time_series_sql = \"\"\"\n",
        "    SELECT \n",
        "        DATE(订单创建时间) as 日期,\n",
        "        COUNT(*) as 订单数,\n",
        "        SUM(总金额) as 总销售额,\n",
        "        AVG(总金额) as 平均订单金额\n",
        "    FROM orders\n",
        "    GROUP BY DATE(订单创建时间)\n",
        "    ORDER BY 日期\n",
        "    \"\"\"\n",
        "    return spark.sql(time_series_sql)\n",
        "\n",
        "# 3.2 销售预测模型\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "\n",
        "def build_sales_prediction_model():\n",
        "    # 准备训练数据\n",
        "    sales_data = spark.sql(\"\"\"\n",
        "        SELECT \n",
        "            DAYOFWEEK(订单创建时间) as 星期,\n",
        "            HOUR(订单创建时间) as 小时,\n",
        "            COUNT(*) as 订单数,\n",
        "            SUM(总金额) as 销售额\n",
        "        FROM orders\n",
        "        GROUP BY DAYOFWEEK(订单创建时间), HOUR(订单创建时间)\n",
        "    \"\"\")\n",
        "    \n",
        "    # 特征工程\n",
        "    assembler = VectorAssembler(\n",
        "        inputCols=[\"星期\", \"小时\"],\n",
        "        outputCol=\"features\"\n",
        "    )\n",
        "    \n",
        "    # 准备训练数据\n",
        "    training_data = assembler.transform(sales_data)\n",
        "    \n",
        "    # 训练模型\n",
        "    lr = LinearRegression(featuresCol=\"features\", labelCol=\"销售额\")\n",
        "    model = lr.fit(training_data)\n",
        "    \n",
        "    return model, training_data\n",
        "\n",
        "# 3.3 季节性分析\n",
        "def analyze_seasonality():\n",
        "    seasonality_sql = \"\"\"\n",
        "    SELECT \n",
        "        MONTH(订单创建时间) as 月份,\n",
        "        DAYOFWEEK(订单创建时间) as 星期,\n",
        "        HOUR(订单创建时间) as 小时,\n",
        "        AVG(总金额) as 平均销售额,\n",
        "        COUNT(*) as 订单数\n",
        "    FROM orders\n",
        "    GROUP BY \n",
        "        MONTH(订单创建时间),\n",
        "        DAYOFWEEK(订单创建时间),\n",
        "        HOUR(订单创建时间)\n",
        "    ORDER BY \n",
        "        月份, 星期, 小时\n",
        "    \"\"\"\n",
        "    return spark.sql(seasonality_sql)\n",
        "\n",
        "# 执行分析\n",
        "print(\"时间序列分析结果：\")\n",
        "analyze_time_series().show()\n",
        "\n",
        "print(\"\\n销售预测模型结果：\")\n",
        "model, training_data = build_sales_prediction_model()\n",
        "predictions = model.transform(training_data)\n",
        "predictions.select(\"星期\", \"小时\", \"销售额\", \"prediction\").show()\n",
        "\n",
        "print(\"\\n季节性分析结果：\")\n",
        "analyze_seasonality().show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. 用户分群分析\n",
        "\n",
        "# 4.1 用户价值分群\n",
        "def analyze_user_value_segments():\n",
        "    # 计算RFM指标\n",
        "    rfm_sql = \"\"\"\n",
        "    WITH user_metrics AS (\n",
        "        SELECT \n",
        "            用户ID,\n",
        "            DATEDIFF(CURRENT_DATE, MAX(订单创建时间)) as Recency,\n",
        "            COUNT(DISTINCT DATE(订单创建时间)) as Frequency,\n",
        "            SUM(总金额) as Monetary\n",
        "        FROM orders\n",
        "        GROUP BY 用户ID\n",
        "    )\n",
        "    SELECT \n",
        "        CASE \n",
        "            WHEN Recency <= 30 AND Frequency >= 10 AND Monetary >= 5000 THEN '高价值'\n",
        "            WHEN Recency <= 90 AND Frequency >= 5 AND Monetary >= 2000 THEN '中价值'\n",
        "            ELSE '低价值'\n",
        "        END as 价值分群,\n",
        "        COUNT(*) as 用户数量,\n",
        "        AVG(Monetary) as 平均消费金额,\n",
        "        AVG(Frequency) as 平均购买频率\n",
        "    FROM user_metrics\n",
        "    GROUP BY \n",
        "        CASE \n",
        "            WHEN Recency <= 30 AND Frequency >= 10 AND Monetary >= 5000 THEN '高价值'\n",
        "            WHEN Recency <= 90 AND Frequency >= 5 AND Monetary >= 2000 THEN '中价值'\n",
        "            ELSE '低价值'\n",
        "        END\n",
        "    \"\"\"\n",
        "    return spark.sql(rfm_sql)\n",
        "\n",
        "# 4.2 用户行为分群\n",
        "def analyze_user_behavior_clusters():\n",
        "    # 准备聚类特征\n",
        "    behavior_data = spark.sql(\"\"\"\n",
        "        SELECT \n",
        "            用户ID,\n",
        "            COUNT(*) as 购买次数,\n",
        "            AVG(总金额) as 平均订单金额,\n",
        "            COUNT(DISTINCT 商品类别) as 购买类别数,\n",
        "            SUM(商品数量) as 总购买数量\n",
        "        FROM orders\n",
        "        GROUP BY 用户ID\n",
        "    \"\"\")\n",
        "    \n",
        "    # 特征标准化\n",
        "    assembler = VectorAssembler(\n",
        "        inputCols=[\"购买次数\", \"平均订单金额\", \"购买类别数\", \"总购买数量\"],\n",
        "        outputCol=\"features\"\n",
        "    )\n",
        "    \n",
        "    # 准备训练数据\n",
        "    training_data = assembler.transform(behavior_data)\n",
        "    \n",
        "    # K-means聚类\n",
        "    kmeans = KMeans(k=3, featuresCol=\"features\")\n",
        "    model = kmeans.fit(training_data)\n",
        "    \n",
        "    # 添加聚类标签\n",
        "    clustered_data = model.transform(training_data)\n",
        "    \n",
        "    # 分析聚类结果\n",
        "    return clustered_data.groupBy(\"prediction\") \\\n",
        "        .agg(\n",
        "            count(\"*\").alias(\"用户数量\"),\n",
        "            avg(\"购买次数\").alias(\"平均购买次数\"),\n",
        "            avg(\"平均订单金额\").alias(\"平均订单金额\"),\n",
        "            avg(\"购买类别数\").alias(\"平均购买类别数\"),\n",
        "            avg(\"总购买数量\").alias(\"平均购买数量\")\n",
        "        )\n",
        "\n",
        "# 4.3 精准营销建议\n",
        "def generate_marketing_recommendations():\n",
        "    marketing_sql = \"\"\"\n",
        "    WITH user_segments AS (\n",
        "        SELECT \n",
        "            用户ID,\n",
        "            CASE \n",
        "                WHEN COUNT(*) >= 10 AND SUM(总金额) >= 5000 THEN '高价值'\n",
        "                WHEN COUNT(*) >= 5 AND SUM(总金额) >= 2000 THEN '中价值'\n",
        "                ELSE '低价值'\n",
        "            END as 用户分群,\n",
        "            MAX(订单创建时间) as 最近购买时间,\n",
        "            COUNT(DISTINCT 商品类别) as 购买类别数,\n",
        "            AVG(总金额) as 平均订单金额\n",
        "        FROM orders\n",
        "        GROUP BY 用户ID\n",
        "    )\n",
        "    SELECT \n",
        "        用户分群,\n",
        "        CASE \n",
        "            WHEN 用户分群 = '高价值' THEN '提供VIP服务和专属优惠'\n",
        "            WHEN 用户分群 = '中价值' THEN '推送相关商品和促销活动'\n",
        "            ELSE '提供新用户优惠和基础服务'\n",
        "        END as 营销建议,\n",
        "        COUNT(*) as 用户数量,\n",
        "        AVG(购买类别数) as 平均购买类别数,\n",
        "        AVG(平均订单金额) as 平均订单金额\n",
        "    FROM user_segments\n",
        "    GROUP BY 用户分群\n",
        "    \"\"\"\n",
        "    return spark.sql(marketing_sql)\n",
        "\n",
        "# 执行分析\n",
        "print(\"用户价值分群分析结果：\")\n",
        "analyze_user_value_segments().show()\n",
        "\n",
        "print(\"\\n用户行为分群分析结果：\")\n",
        "analyze_user_behavior_clusters().show()\n",
        "\n",
        "print(\"\\n精准营销建议：\")\n",
        "generate_marketing_recommendations().show()\n",
        "\n",
        "# 停止Spark会话\n",
        "spark.stop()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# 第六部分：数据可视化\n",
        "\n",
        "在这部分中，我们将使用多种可视化工具来展示分析结果，包括：\n",
        "\n",
        "1. 销售趋势可视化\n",
        "   - 销售额时间序列图\n",
        "   - 订单量趋势图\n",
        "   - 季节性分析图\n",
        "\n",
        "2. 用户分析可视化\n",
        "   - 用户分群分布图\n",
        "   - 用户价值金字塔\n",
        "   - 用户行为雷达图\n",
        "\n",
        "3. 商品分析可视化\n",
        "   - 商品关联网络图\n",
        "   - 品类销售占比图\n",
        "   - 热门商品排行榜\n",
        "\n",
        "4. 地理分布可视化\n",
        "   - 销售地理热力图\n",
        "   - 区域订单分布图\n",
        "   - 物流路线图\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 导入可视化所需的库\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import plotly.figure_factory as ff\n",
        "from plotly.subplots import make_subplots\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pyecharts.options as opts\n",
        "from pyecharts.charts import Bar, Line, Pie, HeatMap, Graph\n",
        "from pyecharts.globals import ThemeType\n",
        "\n",
        "# 重新创建SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"E-commerce Visualization\") \\\n",
        "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# 1. 销售趋势可视化\n",
        "def visualize_sales_trends():\n",
        "    # 获取销售趋势数据\n",
        "    sales_trend = spark.sql(\"\"\"\n",
        "        SELECT \n",
        "            DATE(订单创建时间) as 日期,\n",
        "            COUNT(*) as 订单数,\n",
        "            SUM(总金额) as 销售额\n",
        "        FROM orders\n",
        "        GROUP BY DATE(订单创建时间)\n",
        "        ORDER BY 日期\n",
        "    \"\"\").toPandas()\n",
        "    \n",
        "    # 创建销售趋势图\n",
        "    fig = make_subplots(rows=2, cols=1,\n",
        "                       subplot_titles=('日销售额趋势', '日订单量趋势'))\n",
        "    \n",
        "    # 销售额趋势\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=sales_trend['日期'], y=sales_trend['销售额'],\n",
        "                  mode='lines', name='销售额'),\n",
        "        row=1, col=1\n",
        "    )\n",
        "    \n",
        "    # 订单量趋势\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=sales_trend['日期'], y=sales_trend['订单数'],\n",
        "                  mode='lines', name='订单数'),\n",
        "        row=2, col=1\n",
        "    )\n",
        "    \n",
        "    fig.update_layout(height=800, title_text=\"销售趋势分析\")\n",
        "    fig.show()\n",
        "\n",
        "# 2. 用户分析可视化\n",
        "def visualize_user_analysis():\n",
        "    # 获取用户分群数据\n",
        "    user_segments = spark.sql(\"\"\"\n",
        "        WITH user_metrics AS (\n",
        "            SELECT \n",
        "                用户ID,\n",
        "                COUNT(*) as 购买次数,\n",
        "                SUM(总金额) as 总消费额,\n",
        "                AVG(总金额) as 平均订单额\n",
        "            FROM orders\n",
        "            GROUP BY 用户ID\n",
        "        )\n",
        "        SELECT \n",
        "            CASE \n",
        "                WHEN 总消费额 >= 5000 THEN '高价值'\n",
        "                WHEN 总消费额 >= 2000 THEN '中价值'\n",
        "                ELSE '低价值'\n",
        "            END as 用户分群,\n",
        "            COUNT(*) as 用户数量,\n",
        "            SUM(总消费额) as 分群总消费额\n",
        "        FROM user_metrics\n",
        "        GROUP BY \n",
        "            CASE \n",
        "                WHEN 总消费额 >= 5000 THEN '高价值'\n",
        "                WHEN 总消费额 >= 2000 THEN '中价值'\n",
        "                ELSE '低价值'\n",
        "            END\n",
        "    \"\"\").toPandas()\n",
        "    \n",
        "    # 创建用户价值金字塔\n",
        "    fig = go.Figure(data=[go.Funnel(\n",
        "        y=user_segments['用户分群'],\n",
        "        x=user_segments['用户数量'],\n",
        "        textinfo=\"value+percent initial\"\n",
        "    )])\n",
        "    \n",
        "    fig.update_layout(title_text=\"用户价值金字塔\")\n",
        "    fig.show()\n",
        "\n",
        "# 3. 商品分析可视化\n",
        "def visualize_product_analysis():\n",
        "    # 获取品类销售数据\n",
        "    category_sales = spark.sql(\"\"\"\n",
        "        SELECT \n",
        "            商品类别,\n",
        "            COUNT(*) as 销售量,\n",
        "            SUM(总金额) as 销售额\n",
        "        FROM orders\n",
        "        GROUP BY 商品类别\n",
        "        ORDER BY 销售额 DESC\n",
        "    \"\"\").toPandas()\n",
        "    \n",
        "    # 创建品类销售占比图\n",
        "    fig = px.pie(category_sales, \n",
        "                 values='销售额', \n",
        "                 names='商品类别',\n",
        "                 title='商品类别销售额占比')\n",
        "    fig.show()\n",
        "\n",
        "# 4. 地理分布可视化\n",
        "def visualize_geographic_distribution():\n",
        "    # 获取地理分布数据\n",
        "    geo_distribution = spark.sql(\"\"\"\n",
        "        SELECT \n",
        "            收货地址,\n",
        "            COUNT(*) as 订单数,\n",
        "            SUM(总金额) as 销售额\n",
        "        FROM orders\n",
        "        GROUP BY 收货地址\n",
        "        ORDER BY 销售额 DESC\n",
        "    \"\"\").toPandas()\n",
        "    \n",
        "    # 创建地理分布热力图\n",
        "    c = (\n",
        "        HeatMap()\n",
        "        .add_xaxis(list(geo_distribution['收货地址']))\n",
        "        .add_yaxis(\n",
        "            \"销售额\",\n",
        "            list(geo_distribution['销售额']),\n",
        "            label_opts=opts.LabelOpts(is_show=True, position=\"inside\")\n",
        "        )\n",
        "        .set_global_opts(\n",
        "            title_opts=opts.TitleOpts(title=\"销售地理分布热力图\"),\n",
        "            visualmap_opts=opts.VisualMapOpts()\n",
        "        )\n",
        "    )\n",
        "    c.render_notebook()\n",
        "\n",
        "# 执行可视化\n",
        "print(\"生成销售趋势可视化...\")\n",
        "visualize_sales_trends()\n",
        "\n",
        "print(\"\\n生成用户分析可视化...\")\n",
        "visualize_user_analysis()\n",
        "\n",
        "print(\"\\n生成商品分析可视化...\")\n",
        "visualize_product_analysis()\n",
        "\n",
        "print(\"\\n生成地理分布可视化...\")\n",
        "visualize_geographic_distribution()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# 第七部分：性能优化\n",
        "\n",
        "在这部分中，我们将对系统进行性能优化，包括：\n",
        "\n",
        "1. Spark性能优化\n",
        "   - 内存管理优化\n",
        "   - 分区优化\n",
        "   - 缓存策略优化\n",
        "   - SQL查询优化\n",
        "\n",
        "2. 数据存储优化\n",
        "   - 数据压缩\n",
        "   - 索引优化\n",
        "   - 分区策略\n",
        "   - 存储格式选择\n",
        "\n",
        "3. 实时处理优化\n",
        "   - Kafka配置优化\n",
        "   - Flink并行度调整\n",
        "   - 状态后端优化\n",
        "   - Checkpoint配置\n",
        "\n",
        "4. 系统资源优化\n",
        "   - CPU利用率优化\n",
        "   - 内存分配优化\n",
        "   - 网络传输优化\n",
        "   - 磁盘I/O优化\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Spark性能优化\n",
        "\n",
        "# 优化Spark配置\n",
        "def optimize_spark_config():\n",
        "    # 创建优化后的SparkSession\n",
        "    optimized_spark = SparkSession.builder \\\n",
        "        .appName(\"Optimized E-commerce Analysis\") \\\n",
        "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
        "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "        .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
        "        .config(\"spark.memory.fraction\", \"0.8\") \\\n",
        "        .config(\"spark.memory.storageFraction\", \"0.3\") \\\n",
        "        .config(\"spark.speculation\", \"true\") \\\n",
        "        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
        "        .getOrCreate()\n",
        "    \n",
        "    return optimized_spark\n",
        "\n",
        "# 优化数据缓存策略\n",
        "def optimize_caching():\n",
        "    # 识别频繁使用的数据集\n",
        "    orders_df.cache()\n",
        "    \n",
        "    # 创建优化后的临时视图\n",
        "    orders_df.createOrReplaceTempView(\"optimized_orders\")\n",
        "    \n",
        "    # 对常用的聚合结果进行缓存\n",
        "    daily_sales = spark.sql(\"\"\"\n",
        "        SELECT \n",
        "            DATE(订单创建时间) as 日期,\n",
        "            COUNT(*) as 订单数,\n",
        "            SUM(总金额) as 销售额\n",
        "        FROM optimized_orders\n",
        "        GROUP BY DATE(订单创建时间)\n",
        "    \"\"\")\n",
        "    daily_sales.cache()\n",
        "\n",
        "# 2. 数据存储优化\n",
        "\n",
        "# 优化MySQL配置\n",
        "def optimize_mysql():\n",
        "    # 创建索引\n",
        "    spark.sql(\"\"\"\n",
        "        CREATE INDEX idx_order_time ON orders(订单创建时间);\n",
        "        CREATE INDEX idx_user_id ON orders(用户ID);\n",
        "        CREATE INDEX idx_product_id ON orders(商品ID);\n",
        "    \"\"\")\n",
        "    \n",
        "    # 优化表分区\n",
        "    spark.sql(\"\"\"\n",
        "        ALTER TABLE orders\n",
        "        PARTITION BY RANGE (YEAR(订单创建时间)) (\n",
        "            PARTITION p2023 VALUES LESS THAN (2024),\n",
        "            PARTITION p2024 VALUES LESS THAN (2025)\n",
        "        );\n",
        "    \"\"\")\n",
        "\n",
        "# 3. 实时处理优化\n",
        "\n",
        "# 优化Kafka配置\n",
        "kafka_optimized_config = {\n",
        "    'bootstrap.servers': 'localhost:9092',\n",
        "    'compression.type': 'lz4',\n",
        "    'batch.size': '65536',\n",
        "    'linger.ms': '5',\n",
        "    'buffer.memory': '67108864',\n",
        "    'acks': '1'\n",
        "}\n",
        "\n",
        "# 优化Flink配置\n",
        "def optimize_flink():\n",
        "    # 设置Flink优化参数\n",
        "    env_config = {\n",
        "        'parallelism.default': '4',\n",
        "        'taskmanager.memory.process.size': '4096m',\n",
        "        'taskmanager.numberOfTaskSlots': '8',\n",
        "        'state.backend': 'rocksdb',\n",
        "        'state.checkpoints.dir': 'hdfs://namenode:8020/flink-checkpoints',\n",
        "        'execution.checkpointing.interval': '10000',\n",
        "        'execution.checkpointing.mode': 'EXACTLY_ONCE'\n",
        "    }\n",
        "    \n",
        "    # 应用配置\n",
        "    for key, value in env_config.items():\n",
        "        env.get_config().set_string(key, value)\n",
        "\n",
        "# 4. 系统资源优化\n",
        "\n",
        "# 监控系统资源使用\n",
        "def monitor_system_resources():\n",
        "    import psutil\n",
        "    \n",
        "    # 获取系统资源使用情况\n",
        "    cpu_percent = psutil.cpu_percent(interval=1)\n",
        "    memory_info = psutil.virtual_memory()\n",
        "    disk_io = psutil.disk_io_counters()\n",
        "    network_io = psutil.net_io_counters()\n",
        "    \n",
        "    print(f\"系统资源监控:\")\n",
        "    print(f\"CPU使用率: {cpu_percent}%\")\n",
        "    print(f\"内存使用率: {memory_info.percent}%\")\n",
        "    print(f\"磁盘读写: 读取={disk_io.read_bytes/1024/1024}MB, 写入={disk_io.write_bytes/1024/1024}MB\")\n",
        "    print(f\"网络IO: 发送={network_io.bytes_sent/1024/1024}MB, 接收={network_io.bytes_recv/1024/1024}MB\")\n",
        "\n",
        "# 执行优化\n",
        "print(\"开始执行性能优化...\")\n",
        "\n",
        "print(\"\\n1. 优化Spark配置...\")\n",
        "optimized_spark = optimize_spark_config()\n",
        "optimize_caching()\n",
        "\n",
        "print(\"\\n2. 优化数据存储...\")\n",
        "optimize_mysql()\n",
        "\n",
        "print(\"\\n3. 优化实时处理...\")\n",
        "optimize_flink()\n",
        "\n",
        "print(\"\\n4. 监控系统资源...\")\n",
        "monitor_system_resources()\n",
        "\n",
        "print(\"\\n性能优化完成！\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# 第八部分：系统集成\n",
        "\n",
        "在这部分中，我们将整合所有组件，构建完整的数据处理流水线：\n",
        "\n",
        "1. 数据流集成\n",
        "   - 数据采集层\n",
        "   - 存储层\n",
        "   - 计算层\n",
        "   - 服务层\n",
        "\n",
        "2. 接口集成\n",
        "   - REST API设计\n",
        "   - 数据服务接口\n",
        "   - 监控接口\n",
        "   - 管理接口\n",
        "\n",
        "3. 监控告警\n",
        "   - 系统监控\n",
        "   - 业务监控\n",
        "   - 告警规则\n",
        "   - 通知机制\n",
        "\n",
        "4. 部署方案\n",
        "   - 容器化部署\n",
        "   - 服务编排\n",
        "   - 负载均衡\n",
        "   - 高可用设计\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. 数据流集成\n",
        "\n",
        "from flask import Flask, jsonify, request\n",
        "import json\n",
        "import logging\n",
        "from prometheus_client import start_http_server, Counter, Gauge\n",
        "import docker\n",
        "import yaml\n",
        "\n",
        "# 创建Flask应用\n",
        "app = Flask(__name__)\n",
        "\n",
        "# 设置日志\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# 2. 接口集成\n",
        "\n",
        "# REST API路由\n",
        "@app.route('/api/v1/sales/daily', methods=['GET'])\n",
        "def get_daily_sales():\n",
        "    try:\n",
        "        start_date = request.args.get('start_date')\n",
        "        end_date = request.args.get('end_date')\n",
        "        \n",
        "        sales_data = spark.sql(f\"\"\"\n",
        "            SELECT \n",
        "                DATE(订单创建时间) as 日期,\n",
        "                COUNT(*) as 订单数,\n",
        "                SUM(总金额) as 销售额\n",
        "            FROM orders\n",
        "            WHERE DATE(订单创建时间) BETWEEN '{start_date}' AND '{end_date}'\n",
        "            GROUP BY DATE(订单创建时间)\n",
        "            ORDER BY 日期\n",
        "        \"\"\").toPandas().to_dict('records')\n",
        "        \n",
        "        return jsonify({\n",
        "            'status': 'success',\n",
        "            'data': sales_data\n",
        "        })\n",
        "    except Exception as e:\n",
        "        logger.error(f\"获取日销售数据失败: {str(e)}\")\n",
        "        return jsonify({\n",
        "            'status': 'error',\n",
        "            'message': str(e)\n",
        "        }), 500\n",
        "\n",
        "@app.route('/api/v1/users/segments', methods=['GET'])\n",
        "def get_user_segments():\n",
        "    try:\n",
        "        user_segments = spark.sql(\"\"\"\n",
        "            WITH user_metrics AS (\n",
        "                SELECT \n",
        "                    用户ID,\n",
        "                    COUNT(*) as 购买次数,\n",
        "                    SUM(总金额) as 总消费额\n",
        "                FROM orders\n",
        "                GROUP BY 用户ID\n",
        "            )\n",
        "            SELECT \n",
        "                CASE \n",
        "                    WHEN 总消费额 >= 5000 THEN '高价值'\n",
        "                    WHEN 总消费额 >= 2000 THEN '中价值'\n",
        "                    ELSE '低价值'\n",
        "                END as 用户分群,\n",
        "                COUNT(*) as 用户数量\n",
        "            FROM user_metrics\n",
        "            GROUP BY \n",
        "                CASE \n",
        "                    WHEN 总消费额 >= 5000 THEN '高价值'\n",
        "                    WHEN 总消费额 >= 2000 THEN '中价值'\n",
        "                    ELSE '低价值'\n",
        "                END\n",
        "        \"\"\").toPandas().to_dict('records')\n",
        "        \n",
        "        return jsonify({\n",
        "            'status': 'success',\n",
        "            'data': user_segments\n",
        "        })\n",
        "    except Exception as e:\n",
        "        logger.error(f\"获取用户分群数据失败: {str(e)}\")\n",
        "        return jsonify({\n",
        "            'status': 'error',\n",
        "            'message': str(e)\n",
        "        }), 500\n",
        "\n",
        "# 3. 监控告警\n",
        "\n",
        "# Prometheus指标\n",
        "SALES_COUNTER = Counter('total_sales', 'Total sales amount')\n",
        "ORDER_COUNTER = Counter('total_orders', 'Total number of orders')\n",
        "ACTIVE_USERS = Gauge('active_users', 'Number of active users')\n",
        "\n",
        "def update_metrics():\n",
        "    try:\n",
        "        # 更新销售指标\n",
        "        metrics = spark.sql(\"\"\"\n",
        "            SELECT \n",
        "                SUM(总金额) as total_sales,\n",
        "                COUNT(*) as total_orders,\n",
        "                COUNT(DISTINCT 用户ID) as active_users\n",
        "            FROM orders\n",
        "            WHERE DATE(订单创建时间) = CURRENT_DATE\n",
        "        \"\"\").collect()[0]\n",
        "        \n",
        "        SALES_COUNTER.inc(metrics['total_sales'])\n",
        "        ORDER_COUNTER.inc(metrics['total_orders'])\n",
        "        ACTIVE_USERS.set(metrics['active_users'])\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"更新监控指标失败: {str(e)}\")\n",
        "\n",
        "# 4. 部署方案\n",
        "\n",
        "# Docker配置\n",
        "def generate_docker_compose():\n",
        "    services = {\n",
        "        'spark-master': {\n",
        "            'image': 'bitnami/spark:latest',\n",
        "            'ports': ['8080:8080', '7077:7077'],\n",
        "            'environment': ['SPARK_MODE=master']\n",
        "        },\n",
        "        'spark-worker': {\n",
        "            'image': 'bitnami/spark:latest',\n",
        "            'environment': [\n",
        "                'SPARK_MODE=worker',\n",
        "                'SPARK_MASTER_URL=spark://spark-master:7077'\n",
        "            ],\n",
        "            'depends_on': ['spark-master']\n",
        "        },\n",
        "        'mysql': {\n",
        "            'image': 'mysql:8.0',\n",
        "            'environment': [\n",
        "                'MYSQL_ROOT_PASSWORD=your_password',\n",
        "                'MYSQL_DATABASE=ecommerce'\n",
        "            ],\n",
        "            'volumes': ['mysql_data:/var/lib/mysql']\n",
        "        },\n",
        "        'kafka': {\n",
        "            'image': 'bitnami/kafka:latest',\n",
        "            'environment': [\n",
        "                'KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181',\n",
        "                'ALLOW_PLAINTEXT_LISTENER=yes'\n",
        "            ],\n",
        "            'depends_on': ['zookeeper']\n",
        "        },\n",
        "        'zookeeper': {\n",
        "            'image': 'bitnami/zookeeper:latest',\n",
        "            'environment': ['ALLOW_ANONYMOUS_LOGIN=yes']\n",
        "        },\n",
        "        'flink-jobmanager': {\n",
        "            'image': 'flink:latest',\n",
        "            'ports': ['8081:8081'],\n",
        "            'command': 'jobmanager',\n",
        "            'environment': ['JOB_MANAGER_RPC_ADDRESS=flink-jobmanager']\n",
        "        },\n",
        "        'flink-taskmanager': {\n",
        "            'image': 'flink:latest',\n",
        "            'command': 'taskmanager',\n",
        "            'environment': ['JOB_MANAGER_RPC_ADDRESS=flink-jobmanager'],\n",
        "            'depends_on': ['flink-jobmanager']\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    compose = {\n",
        "        'version': '3',\n",
        "        'services': services,\n",
        "        'volumes': {\n",
        "            'mysql_data': None\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    with open('docker-compose.yml', 'w') as f:\n",
        "        yaml.dump(compose, f)\n",
        "\n",
        "# 启动系统\n",
        "def start_system():\n",
        "    try:\n",
        "        # 启动Prometheus监控\n",
        "        start_http_server(8000)\n",
        "        \n",
        "        # 生成Docker配置\n",
        "        generate_docker_compose()\n",
        "        \n",
        "        # 启动Flask应用\n",
        "        app.run(host='0.0.0.0', port=5000)\n",
        "        \n",
        "        logger.info(\"系统已成功启动！\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"系统启动失败: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    start_system()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# 第九部分：文档和报告\n",
        "\n",
        "## 1. 项目概述\n",
        "\n",
        "本项目是一个基于大数据技术栈的电商数据分析系统，主要功能包括：\n",
        "\n",
        "- 数据采集和预处理\n",
        "- 数据存储和管理\n",
        "- 批处理分析\n",
        "- 实时处理\n",
        "- 复杂分析\n",
        "- 数据可视化\n",
        "- 系统集成\n",
        "\n",
        "## 2. 系统架构\n",
        "\n",
        "系统采用分布式架构，主要组件包括：\n",
        "\n",
        "1. 数据采集层\n",
        "   - Kafka用于实时数据采集\n",
        "   - Flume用于日志收集\n",
        "\n",
        "2. 存储层\n",
        "   - MySQL用于结构化数据存储\n",
        "   - HDFS用于大规模数据存储\n",
        "   - Redis用于缓存\n",
        "\n",
        "3. 计算层\n",
        "   - Spark用于批处理分析\n",
        "   - Flink用于实时处理\n",
        "   - Hadoop用于资源调度\n",
        "\n",
        "4. 服务层\n",
        "   - Flask提供REST API\n",
        "   - Prometheus进行监控\n",
        "   - Docker进行容器化部署\n",
        "\n",
        "## 3. 主要功能\n",
        "\n",
        "1. 用户分析\n",
        "   - 用户行为分析\n",
        "   - 用户价值分析\n",
        "   - 用户生命周期管理\n",
        "\n",
        "2. 商品分析\n",
        "   - 商品关联分析\n",
        "   - 销售趋势分析\n",
        "   - 商品推荐\n",
        "\n",
        "3. 实时监控\n",
        "   - 销售额监控\n",
        "   - 订单量监控\n",
        "   - 用户活跃度监控\n",
        "\n",
        "4. 运营分析\n",
        "   - ROI分析\n",
        "   - 营销效果分析\n",
        "   - 库存优化建议\n",
        "\n",
        "## 4. 部署说明\n",
        "\n",
        "1. 环境要求\n",
        "   - Python 3.8+\n",
        "   - Java 8+\n",
        "   - Docker 20.10+\n",
        "   - 16GB+ RAM\n",
        "   - 4+ CPU cores\n",
        "\n",
        "2. 安装步骤\n",
        "   ```bash\n",
        "   # 1. 克隆代码\n",
        "   git clone https://github.com/your-repo/ecommerce-analysis.git\n",
        "   \n",
        "   # 2. 安装依赖\n",
        "   pip install -r requirements.txt\n",
        "   \n",
        "   # 3. 启动服务\n",
        "   docker-compose up -d\n",
        "   \n",
        "   # 4. 初始化数据库\n",
        "   python init_db.py\n",
        "   \n",
        "   # 5. 启动应用\n",
        "   python app.py\n",
        "   ```\n",
        "\n",
        "3. 配置说明\n",
        "   - 修改config.yaml进行系统配置\n",
        "   - 修改.env设置环境变量\n",
        "   - 修改docker-compose.yml配置容器\n",
        "\n",
        "## 5. 使用说明\n",
        "\n",
        "1. 访问地址\n",
        "   - Web界面：http://localhost:5000\n",
        "   - API文档：http://localhost:5000/api/docs\n",
        "   - 监控面板：http://localhost:8000\n",
        "\n",
        "2. API使用\n",
        "   ```python\n",
        "   # 获取销售数据\n",
        "   GET /api/v1/sales/daily?start_date=2024-01-01&end_date=2024-01-31\n",
        "   \n",
        "   # 获取用户分群\n",
        "   GET /api/v1/users/segments\n",
        "   ```\n",
        "\n",
        "3. 监控指标\n",
        "   - total_sales：总销售额\n",
        "   - total_orders：总订单数\n",
        "   - active_users：活跃用户数\n",
        "\n",
        "## 6. 性能指标\n",
        "\n",
        "1. 系统性能\n",
        "   - 支持每秒1000+订单处理\n",
        "   - 毫秒级实时分析响应\n",
        "   - 支持TB级数据存储\n",
        "\n",
        "2. 可用性指标\n",
        "   - 系统可用性99.9%\n",
        "   - 数据一致性99.99%\n",
        "   - 故障恢复时间<5分钟\n",
        "\n",
        "## 7. 维护建议\n",
        "\n",
        "1. 日常维护\n",
        "   - 定期备份数据\n",
        "   - 监控系统资源\n",
        "   - 清理历史数据\n",
        "\n",
        "2. 故障处理\n",
        "   - 检查日志文件\n",
        "   - 重启相关服务\n",
        "   - 回滚配置\n",
        "\n",
        "3. 升级建议\n",
        "   - 先测试后升级\n",
        "   - 保持版本兼容\n",
        "   - 做好备份\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# 第九部分：机器学习与智能分析\n",
        "\n",
        "在这部分中，我们将使用机器学习算法进行更深入的数据分析，包括：\n",
        "\n",
        "1. 销售预测\n",
        "   - 时间序列预测\n",
        "   - 多因素回归分析\n",
        "   - 预测模型评估\n",
        "\n",
        "2. 用户行为预测\n",
        "   - 购买倾向预测\n",
        "   - 流失风险预测\n",
        "   - 生命周期预测\n",
        "\n",
        "3. 智能推荐\n",
        "   - 协同过滤推荐\n",
        "   - 基于内容推荐\n",
        "   - 混合推荐算法\n",
        "\n",
        "4. 异常检测\n",
        "   - 交易异常检测\n",
        "   - 用户行为异常\n",
        "   - 系统异常监控\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 导入机器学习相关库\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
        "from pyspark.ml.regression import LinearRegression, RandomForestRegressor\n",
        "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.evaluation import RegressionEvaluator, BinaryClassificationEvaluator\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.ml.pipeline import Pipeline\n",
        "from pyspark.sql.functions import col, datediff, current_date\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import xgboost as xgb\n",
        "from prophet import Prophet\n",
        "\n",
        "# 1. 销售预测\n",
        "def sales_prediction():\n",
        "    # 准备时间序列数据\n",
        "    sales_data = spark.sql(\"\"\"\n",
        "        SELECT \n",
        "            DATE(订单创建时间) as ds,\n",
        "            SUM(总金额) as y,\n",
        "            COUNT(*) as order_count,\n",
        "            COUNT(DISTINCT 用户ID) as user_count,\n",
        "            AVG(总金额) as avg_amount\n",
        "        FROM orders\n",
        "        GROUP BY DATE(订单创建时间)\n",
        "        ORDER BY ds\n",
        "    \"\"\").toPandas()\n",
        "    \n",
        "    # 使用Prophet进行时间序列预测\n",
        "    model = Prophet(yearly_seasonality=True, weekly_seasonality=True)\n",
        "    model.fit(sales_data[['ds', 'y']])\n",
        "    \n",
        "    # 预测未来30天\n",
        "    future = model.make_future_dataframe(periods=30)\n",
        "    forecast = model.predict(future)\n",
        "    \n",
        "    # 多因素回归预测\n",
        "    feature_cols = ['order_count', 'user_count', 'avg_amount']\n",
        "    assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
        "    scaler = StandardScaler(inputCol='features', outputCol='scaled_features')\n",
        "    lr = LinearRegression(featuresCol='scaled_features', labelCol='y')\n",
        "    \n",
        "    pipeline = Pipeline(stages=[assembler, scaler, lr])\n",
        "    model = pipeline.fit(spark.createDataFrame(sales_data))\n",
        "    \n",
        "    return forecast, model\n",
        "\n",
        "# 2. 用户行为预测\n",
        "def user_behavior_prediction():\n",
        "    # 准备用户特征\n",
        "    user_features = spark.sql(\"\"\"\n",
        "        SELECT \n",
        "            用户ID,\n",
        "            COUNT(*) as 购买次数,\n",
        "            SUM(总金额) as 总消费额,\n",
        "            AVG(总金额) as 平均订单金额,\n",
        "            DATEDIFF(CURRENT_DATE, MAX(订单创建时间)) as 最近购买天数,\n",
        "            COUNT(DISTINCT 商品类别) as 购买类别数\n",
        "        FROM orders\n",
        "        GROUP BY 用户ID\n",
        "    \"\"\")\n",
        "    \n",
        "    # 定义流失标签（90天未购买视为流失）\n",
        "    user_features = user_features.withColumn(\n",
        "        'is_churn',\n",
        "        col('最近购买天数') > 90\n",
        "    )\n",
        "    \n",
        "    # 准备特征\n",
        "    feature_cols = ['购买次数', '总消费额', '平均订单金额', '最近购买天数', '购买类别数']\n",
        "    assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
        "    \n",
        "    # 训练流失预测模型\n",
        "    rf = RandomForestClassifier(\n",
        "        featuresCol='features',\n",
        "        labelCol='is_churn',\n",
        "        numTrees=100\n",
        "    )\n",
        "    \n",
        "    pipeline = Pipeline(stages=[assembler, rf])\n",
        "    model = pipeline.fit(user_features)\n",
        "    \n",
        "    return model\n",
        "\n",
        "# 3. 智能推荐\n",
        "def build_recommendation_system():\n",
        "    # 准备用户-商品评分数据\n",
        "    ratings = spark.sql(\"\"\"\n",
        "        SELECT \n",
        "            用户ID as user,\n",
        "            商品ID as item,\n",
        "            总金额 as rating\n",
        "        FROM orders\n",
        "    \"\"\")\n",
        "    \n",
        "    # 使用ALS算法构建推荐系统\n",
        "    als = ALS(\n",
        "        maxIter=10,\n",
        "        regParam=0.01,\n",
        "        userCol=\"user\",\n",
        "        itemCol=\"item\",\n",
        "        ratingCol=\"rating\",\n",
        "        coldStartStrategy=\"drop\"\n",
        "    )\n",
        "    \n",
        "    model = als.fit(ratings)\n",
        "    \n",
        "    # 生成推荐\n",
        "    userRecs = model.recommendForAllUsers(10)\n",
        "    itemRecs = model.recommendForAllItems(10)\n",
        "    \n",
        "    return model, userRecs, itemRecs\n",
        "\n",
        "# 4. 异常检测\n",
        "def anomaly_detection():\n",
        "    # 交易异常检测\n",
        "    transaction_anomalies = spark.sql(\"\"\"\n",
        "        WITH user_stats AS (\n",
        "            SELECT \n",
        "                用户ID,\n",
        "                AVG(总金额) as avg_amount,\n",
        "                STDDEV(总金额) as std_amount\n",
        "            FROM orders\n",
        "            GROUP BY 用户ID\n",
        "        )\n",
        "        SELECT \n",
        "            o.*,\n",
        "            (o.总金额 - us.avg_amount) / us.std_amount as z_score\n",
        "        FROM orders o\n",
        "        JOIN user_stats us ON o.用户ID = us.用户ID\n",
        "        WHERE ABS((o.总金额 - us.avg_amount) / us.std_amount) > 3\n",
        "    \"\"\")\n",
        "    \n",
        "    # 用户行为异常检测\n",
        "    behavior_anomalies = spark.sql(\"\"\"\n",
        "        WITH user_daily_stats AS (\n",
        "            SELECT \n",
        "                用户ID,\n",
        "                DATE(订单创建时间) as date,\n",
        "                COUNT(*) as daily_orders,\n",
        "                SUM(总金额) as daily_amount\n",
        "            FROM orders\n",
        "            GROUP BY 用户ID, DATE(订单创建时间)\n",
        "        ),\n",
        "        user_averages AS (\n",
        "            SELECT \n",
        "                用户ID,\n",
        "                AVG(daily_orders) as avg_daily_orders,\n",
        "                STDDEV(daily_orders) as std_daily_orders,\n",
        "                AVG(daily_amount) as avg_daily_amount,\n",
        "                STDDEV(daily_amount) as std_daily_amount\n",
        "            FROM user_daily_stats\n",
        "            GROUP BY 用户ID\n",
        "        )\n",
        "        SELECT \n",
        "            uds.*,\n",
        "            (uds.daily_orders - ua.avg_daily_orders) / ua.std_daily_orders as order_z_score,\n",
        "            (uds.daily_amount - ua.avg_daily_amount) / ua.std_daily_amount as amount_z_score\n",
        "        FROM user_daily_stats uds\n",
        "        JOIN user_averages ua ON uds.用户ID = ua.用户ID\n",
        "        WHERE \n",
        "            ABS((uds.daily_orders - ua.avg_daily_orders) / ua.std_daily_orders) > 3\n",
        "            OR ABS((uds.daily_amount - ua.avg_daily_amount) / ua.std_daily_amount) > 3\n",
        "    \"\"\")\n",
        "    \n",
        "    return transaction_anomalies, behavior_anomalies\n",
        "\n",
        "# 执行分析\n",
        "print(\"开始执行机器学习分析...\")\n",
        "\n",
        "print(\"\\n1. 执行销售预测...\")\n",
        "forecast, sales_model = sales_prediction()\n",
        "\n",
        "print(\"\\n2. 执行用户行为预测...\")\n",
        "user_model = user_behavior_prediction()\n",
        "\n",
        "print(\"\\n3. 构建推荐系统...\")\n",
        "rec_model, user_recommendations, item_recommendations = build_recommendation_system()\n",
        "\n",
        "print(\"\\n4. 执行异常检测...\")\n",
        "trans_anomalies, behav_anomalies = anomaly_detection()\n",
        "\n",
        "print(\"\\n机器学习分析完成！\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# 电商订单数据分析项目\n",
        "\n",
        "## 第一部分：数据采集和预处理\n",
        "\n",
        "本项目将分析电商订单数据，包括：\n",
        "1. 数据理解与检查\n",
        "2. 数据清洗\n",
        "3. 特征工程\n",
        "4. 数据转换\n",
        "5. 数据质量验证\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 导入所需的库\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 设置中文显示\n",
        "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "# 读取数据\n",
        "df = pd.read_csv('tmall_order_report.csv')\n",
        "print(\"数据集基本信息：\")\n",
        "print(f\"数据集大小：{df.shape}\")\n",
        "print(\"\\n数据集前5行：\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 数据质量检查\n",
        "print(\"数据集信息：\")\n",
        "df.info()\n",
        "\n",
        "print(\"\\n缺失值统计：\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "print(\"\\n重复值统计：\")\n",
        "print(f\"重复行数：{df.duplicated().sum()}\")\n",
        "\n",
        "print(\"\\n数据基本统计：\")\n",
        "df.describe()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 数据清洗\n",
        "def clean_data(df):\n",
        "    # 复制数据框\n",
        "    df_clean = df.copy()\n",
        "    \n",
        "    # 1. 处理时间列\n",
        "    df_clean['订单创建时间'] = pd.to_datetime(df_clean['订单创建时间'])\n",
        "    df_clean['订单付款时间'] = pd.to_datetime(df_clean['订单付款时间'])\n",
        "    \n",
        "    # 2. 处理地址列\n",
        "    df_clean['收货地址'] = df_clean['收货地址'].str.replace('自治区|维吾尔|回族|壮族|省', '')\n",
        "    \n",
        "    # 3. 添加新的特征\n",
        "    # 计算订单支付时长（分钟）\n",
        "    df_clean['支付时长'] = (df_clean['订单付款时间'] - df_clean['订单创建时间']).dt.total_seconds() / 60\n",
        "    \n",
        "    # 添加时间相关特征\n",
        "    df_clean['下单年月'] = df_clean['订单创建时间'].dt.strftime('%Y-%m')\n",
        "    df_clean['下单时间'] = df_clean['订单创建时间'].dt.hour\n",
        "    df_clean['下单星期'] = df_clean['订单创建时间'].dt.dayofweek\n",
        "    \n",
        "    # 4. 计算订单状态\n",
        "    df_clean['订单状态'] = '已付款'\n",
        "    df_clean.loc[df_clean['订单付款时间'].isnull(), '订单状态'] = '未付款'\n",
        "    df_clean.loc[df_clean['退款金额'] > 0, '订单状态'] = '已退款'\n",
        "    \n",
        "    # 5. 计算实际收入\n",
        "    df_clean['实际收入'] = df_clean['买家实际支付金额'] - df_clean['退款金额']\n",
        "    \n",
        "    return df_clean\n",
        "\n",
        "# 清洗数据\n",
        "df_clean = clean_data(df)\n",
        "print(\"清洗后的数据集信息：\")\n",
        "df_clean.info()\n",
        "\n",
        "print(\"\\n清洗后的数据样例：\")\n",
        "df_clean.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 数据质量验证\n",
        "def validate_data_quality(df):\n",
        "    print(\"1. 数值范围检查\")\n",
        "    print(\"\\n金额字段的范围：\")\n",
        "    for col in ['总金额', '买家实际支付金额', '退款金额', '实际收入']:\n",
        "        print(f\"{col}范围: {df[col].min():.2f} - {df[col].max():.2f}\")\n",
        "    \n",
        "    print(\"\\n2. 逻辑关系验证\")\n",
        "    # 验证实际支付金额不应大于总金额\n",
        "    invalid_payment = df[df['买家实际支付金额'] > df['总金额']].shape[0]\n",
        "    print(f\"实际支付金额大于总金额的订单数: {invalid_payment}\")\n",
        "    \n",
        "    # 验证退款金额不应大于实际支付金额\n",
        "    invalid_refund = df[df['退款金额'] > df['买家实际支付金额']].shape[0]\n",
        "    print(f\"退款金额大于实际支付金额的订单数: {invalid_refund}\")\n",
        "    \n",
        "    print(\"\\n3. 时间合理性检查\")\n",
        "    # 检查付款时间是否晚于创建时间\n",
        "    invalid_time = df[\n",
        "        (df['订单付款时间'].notna()) & \n",
        "        (df['订单付款时间'] < df['订单创建时间'])\n",
        "    ].shape[0]\n",
        "    print(f\"付款时间早于创建时间的订单数: {invalid_time}\")\n",
        "    \n",
        "    print(\"\\n4. 地址完整性检查\")\n",
        "    print(\"收货地址唯一值：\")\n",
        "    print(df['收货地址'].unique())\n",
        "\n",
        "# 验证数据质量\n",
        "validate_data_quality(df_clean)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 第二部分：数据存储和管理(MySQL)\n",
        "\n",
        "在这部分中，我们将：\n",
        "1. 创建数据库连接\n",
        "2. 设计并创建数据表\n",
        "3. 导入处理后的数据\n",
        "4. 编写基础查询\n",
        "5. 验证数据完整性\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 安装并导入必要的库\n",
        "import mysql.connector\n",
        "from sqlalchemy import create_engine\n",
        "import pymysql\n",
        "import pandas as pd\n",
        "\n",
        "# 数据库连接配置\n",
        "db_config = {\n",
        "    'host': 'localhost',\n",
        "    'user': 'root',\n",
        "    'password': 'your_password',  # 请替换为你的实际密码\n",
        "    'database': 'ecommerce_analysis'\n",
        "}\n",
        "\n",
        "# 创建数据库连接\n",
        "def create_database():\n",
        "    try:\n",
        "        # 创建到MySQL服务器的连接\n",
        "        conn = mysql.connector.connect(\n",
        "            host=db_config['host'],\n",
        "            user=db_config['user'],\n",
        "            password=db_config['password']\n",
        "        )\n",
        "        cursor = conn.cursor()\n",
        "        \n",
        "        # 创建数据库\n",
        "        cursor.execute(f\"CREATE DATABASE IF NOT EXISTS {db_config['database']}\")\n",
        "        print(f\"数据库 {db_config['database']} 创建成功\")\n",
        "        \n",
        "        # 关闭连接\n",
        "        cursor.close()\n",
        "        conn.close()\n",
        "        \n",
        "    except mysql.connector.Error as err:\n",
        "        print(f\"错误: {err}\")\n",
        "\n",
        "# 创建数据表\n",
        "def create_tables():\n",
        "    try:\n",
        "        # 连接到特定数据库\n",
        "        conn = mysql.connector.connect(**db_config)\n",
        "        cursor = conn.cursor()\n",
        "        \n",
        "        # 创建订单表\n",
        "        create_orders_table = \"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS orders (\n",
        "            订单编号 INT PRIMARY KEY,\n",
        "            总金额 DECIMAL(10,2),\n",
        "            买家实际支付金额 DECIMAL(10,2),\n",
        "            收货地址 VARCHAR(50),\n",
        "            订单创建时间 DATETIME,\n",
        "            订单付款时间 DATETIME,\n",
        "            退款金额 DECIMAL(10,2),\n",
        "            支付时长 FLOAT,\n",
        "            下单年月 VARCHAR(7),\n",
        "            下单时间 INT,\n",
        "            下单星期 INT,\n",
        "            订单状态 VARCHAR(10),\n",
        "            实际收入 DECIMAL(10,2)\n",
        "        )\n",
        "        \"\"\"\n",
        "        cursor.execute(create_orders_table)\n",
        "        print(\"订单表创建成功\")\n",
        "        \n",
        "        # 关闭连接\n",
        "        cursor.close()\n",
        "        conn.close()\n",
        "        \n",
        "    except mysql.connector.Error as err:\n",
        "        print(f\"错误: {err}\")\n",
        "\n",
        "# 执行数据库和表的创建\n",
        "create_database()\n",
        "create_tables()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 将数据导入MySQL\n",
        "def import_data_to_mysql(df):\n",
        "    try:\n",
        "        # 创建SQLAlchemy引擎\n",
        "        engine = create_engine(\n",
        "            f\"mysql+pymysql://{db_config['user']}:{db_config['password']}@{db_config['host']}/{db_config['database']}\"\n",
        "        )\n",
        "        \n",
        "        # 将DataFrame导入MySQL\n",
        "        df.to_sql('orders', engine, if_exists='replace', index=False)\n",
        "        print(\"数据成功导入MySQL\")\n",
        "        \n",
        "        # 验证数据导入\n",
        "        with engine.connect() as conn:\n",
        "            result = conn.execute(\"SELECT COUNT(*) FROM orders\").fetchone()\n",
        "            print(f\"导入的记录数: {result[0]}\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"错误: {e}\")\n",
        "\n",
        "# 导入数据\n",
        "import_data_to_mysql(df_clean)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 基础SQL查询示例\n",
        "def run_sql_queries():\n",
        "    try:\n",
        "        # 创建数据库连接\n",
        "        conn = mysql.connector.connect(**db_config)\n",
        "        cursor = conn.cursor()\n",
        "        \n",
        "        # 1. 按订单状态统计订单数量和金额\n",
        "        print(\"1. 订单状态统计：\")\n",
        "        cursor.execute(\"\"\"\n",
        "            SELECT \n",
        "                订单状态,\n",
        "                COUNT(*) as 订单数量,\n",
        "                SUM(总金额) as 总金额,\n",
        "                AVG(总金额) as 平均金额\n",
        "            FROM orders\n",
        "            GROUP BY 订单状态\n",
        "        \"\"\")\n",
        "        print(pd.DataFrame(cursor.fetchall(), \n",
        "                         columns=['订单状态', '订单数量', '总金额', '平均金额']))\n",
        "        \n",
        "        # 2. 按地区统计订单数量\n",
        "        print(\"\\n2. 地区订单统计：\")\n",
        "        cursor.execute(\"\"\"\n",
        "            SELECT \n",
        "                收货地址,\n",
        "                COUNT(*) as 订单数量,\n",
        "                SUM(实际收入) as 总收入\n",
        "            FROM orders\n",
        "            GROUP BY 收货地址\n",
        "            ORDER BY 订单数量 DESC\n",
        "            LIMIT 10\n",
        "        \"\"\")\n",
        "        print(pd.DataFrame(cursor.fetchall(), \n",
        "                         columns=['收货地址', '订单数量', '总收入']))\n",
        "        \n",
        "        # 3. 按时间段统计订单数量\n",
        "        print(\"\\n3. 时间段订单统计：\")\n",
        "        cursor.execute(\"\"\"\n",
        "            SELECT \n",
        "                下单时间,\n",
        "                COUNT(*) as 订单数量\n",
        "            FROM orders\n",
        "            GROUP BY 下单时间\n",
        "            ORDER BY 下单时间\n",
        "        \"\"\")\n",
        "        print(pd.DataFrame(cursor.fetchall(), \n",
        "                         columns=['小时', '订单数量']))\n",
        "        \n",
        "        # 关闭连接\n",
        "        cursor.close()\n",
        "        conn.close()\n",
        "        \n",
        "    except mysql.connector.Error as err:\n",
        "        print(f\"错误: {err}\")\n",
        "\n",
        "# 执行SQL查询\n",
        "run_sql_queries()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 第二部分总结\n",
        "\n",
        "在这部分中，我们完成了：\n",
        "\n",
        "1. 数据库环境搭建\n",
        "   - 创建了ecommerce_analysis数据库\n",
        "   - 设计并创建了orders表结构\n",
        "\n",
        "2. 数据导入\n",
        "   - 将预处理后的数据成功导入MySQL\n",
        "   - 验证了数据的完整性\n",
        "\n",
        "3. 基础分析\n",
        "   - 实现了订单状态统计\n",
        "   - 完成了地区订单分析\n",
        "   - 进行了时间维度分析\n",
        "\n",
        "下一步，我们将进入第三部分：批处理分析(Hadoop)，在那里我们将处理更大规模的数据。\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
